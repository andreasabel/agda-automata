\nonstopmode
\documentclass[preprint,12pt,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{ccicons}
\usepackage{xspace}

\usepackage{bbm}
\usepackage[usenames,dvipsnames]{color}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=Red, citecolor=ForestGreen,
  urlcolor=RoyalBlue}
\usepackage{natbib}
\usepackage[postscript]{ucs}
% Andreas: auto-generated not needed now
% \usepackage[postscript,autogenerated]{ucs}
\usepackage{pifont}
\usepackage{textgreek}
\usepackage[utf8x]{inputenc}
\usepackage{src/latex/agda}
\AgdaNoSpaceAroundCode{}

\DeclareUnicodeCharacter{"02E1}{\ensuremath{{}^{\mathsf{l}}}}
\DeclareUnicodeCharacter{"02E2}{\ensuremath{{}^{\mathsf{s}}}}
\DeclareUnicodeCharacter{"03B4}{\ensuremath{\delta}}
\DeclareUnicodeCharacter{"03B5}{\ensuremath{\varepsilon}}
\DeclareUnicodeCharacter{"03BB}{\ensuremath{\lambda}}
\DeclareUnicodeCharacter{"03BD}{\ensuremath{\nu}}
\DeclareUnicodeCharacter{"1D9C}{\ensuremath{{}^{\mathsf{c}}}}
%\DeclareUnicodeCharacter{"1D52}{\ensuremath{{}^{\mathrm{o}}}} % UNUSED
\DeclareUnicodeCharacter{"1D62}{\ensuremath{{}_i}}
\DeclareUnicodeCharacter{"2032}{\kern0.07em\ensuremath{'}}
\DeclareUnicodeCharacter{"2092}{\ensuremath{{}_o}}
% ALT: rm variant (Andreas: I think italics looks better)
%\DeclareUnicodeCharacter{"1D62}{\ensuremath{{}_{\mathrm{i}}}}
%\DeclareUnicodeCharacter{"2092}{\ensuremath{{}_{\mathrm{o}}}}  % or italics?
\DeclareUnicodeCharacter{"2099}{\ensuremath{{}_{n}}} % here we use italics
\DeclareUnicodeCharacter{"2200}{\ensuremath{\forall}}
\DeclareUnicodeCharacter{"2205}{\ensuremath{\emptyset}}
\DeclareUnicodeCharacter{"220B}{\ensuremath{\ni}}
\DeclareUnicodeCharacter{"220E}{\ensuremath{\scriptstyle{\blacksquare}}}
\DeclareUnicodeCharacter{"2237}{\ensuremath{::}}
\DeclareUnicodeCharacter{"2227}{\ensuremath{\wedge}}
\DeclareUnicodeCharacter{"2228}{\ensuremath{\vee}}
\DeclareUnicodeCharacter{"222A}{\ensuremath{\cup}}
\DeclareUnicodeCharacter{"25B9}{\ensuremath{\vartriangleright}}
\DeclareUnicodeCharacter{"225F}{\ensuremath{\stackrel{?}{=}}}

\usepackage[all]{xy}
\input{macros}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{JLAMP}
%\journal{Journal of Logic and Algebraic Methods in Programming}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{Equational Reasoning about Formal Languages in Coalgebraic
  Style
}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}

\author{Andreas Abel}

\address{Department of Computer Science and Engineering, Gothenburg University}


\begin{abstract}

\end{abstract}

\begin{keyword}
Coalgebra
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
\end{keyword}

\end{frontmatter}

%\tnotetext{}
\ccbyncnd{} This paper is published under a CC-BY-NC-ND license.

%% \linenumbers

\section{Introduction}
\label{sec:intro}

Formal languages and automata are a foundational topic of computer science, with
many practical applications such as compiler construction, textual search, model
checking, and decidability of certain logics.
Automata are an instance of transitions systems which have the
structure of a coalgebra.
Coalgebraic and coinductive reasoning tools such as simulations and bisimulations have
been successfully employed to study formal languages. % \cite{}.
Coinduction-up-to techniques have emerged as the state of the art to
get coinductive proofs through. % \cite{sangiorgi}.

This paper presents an anti-thesis to up-to techniques.  We show how
much of the reasoning about formal languages can be carried out already with
the coinductive notion of equality of languages aka bisimilarity.  We
formalize a coinductive type of languages and the coinductive type
family of strong bisimilarity of languages in Agda using sized types.
The sized typing enables us to do coinductive proofs of bisimilarity
by equational reasoning as customary to establish algebraic
properties.  In particular, after establishing that formal languages
form a Kleene algebra, the laws of the Kleene algebra are sufficient
to prove correctness of the usual automata constructions.


\section{Preliminaries: Type Theory and Agda}
\label{sec:prelim}

\input{latex/Agda}

The results definitions and theorems of this article are formulated in
Dependent Type Theory \`a la \cite{martinlof:predicative75}, in
particular, in the Agda \citep{agdawiki} language.  Agda is an
implementation of Type Theory with several extensions; we will most
notably make use of \emph{sized types}
\citep{hughesParetoSabry:popl96,abelPientka:icfp13} to express
coinductive types and definitions by coinduction.

On the one hand, Agda is a dependently-typed purely functional programming
language, and on the other hand, thanks to the Curry-Howard
correspondence, a proof assistant for constructive logic.  In the
following, we will briefly introduce the syntax of Agda by example.
The experienced Agda user can safely skip the remainder of this section.

% - Curry-Howard isomorphism
% - function computable

\subsection{Agda as a dependently-typed functional language}

The core features of Agda are inductive families
\citep{dybjer:inductivefamilies} and functions defined by pattern
matching.  A very simple inductive family is the enumeration type
$\Bool$ with two constructors $\ttrue$ and $\tfalse$.

\abool

\noindent
$\Bool$ itself has type $\Set$, which is universe or a type of types, but not all
types, to avoid vicious cycles.  For instance $\Set$ itself does not
have type $\Set$, but inhabits the next universe $\agda{Set\ensuremath{_1}}$.
Boolean negation can be defined by simple pattern matching.
\anot

%\begin{minipage}{0.5\linewidth}
%\end{minipage}
%\begin{minipage}{0.5\linewidth}
%\end{minipage}

\noindent
Agda also supports Unicode and infix and mixfix operators.
For example, we can define boolean disjunction like this:

\avee

The notation $(a\;b : \Bool) \to \Bool$ is short
for $(a : \Bool) (b : \Bool) \to \Bool$
or $(a : \Bool) \to (b : \Bool) \to \Bool$,
which is the syntax for dependent function types.
In this case, there is no actual dependency,
since the bound variables $a$ and $b$ are not
subsequently used in the type.


Data types can be parameterized over other types $A$.  For instance
$\Maybe\,A$ embeds an arbitrary type $A$ via $\tjust$ and extends it
by a new value $\tnothing$.

\amaybe

Data types can be recursive, such as $\List\,i\,A$ parameterized by a
size $i$ and a type $A$.  The size $i : \Size$ acts as an upper
bound on the length of the list.  The special size $\tinfty : \Size$
means unbounded length.

\alist

\noindent
A list is either empty (constructor $\tnil$); then any size $i$ is an
upper bound on its length.  Or, the list is nonempty (\eg,
$x \tcons \vxs$);  then, if $j$ is an upper bound on the length of
its tail $\vxs$ and $j < i$, then $i$ is an upper bound on the length
of $x \tcons \vxs$.  Strictly speaking, the constructor $\tconsu$
takes three arguments: $j : \SizeLt i$ and $x : A$ and $\vxs :
\List\,A\,j$, but the first argument $j$ is in $\{$braces$\}$ and thus
declared as hidden.  The user does not have to write it, and its value
will be inferred by Agda if possible.  Note that $j$ occurs in the
type of $\vxs$, thus, this is a proper dependency.

It is possible to supply a hidden argument to a function by enclosing it in
braces.  In case of the infix operator $\tconsu$, we have to fall back
to prefix style $\tconsu\,\{j\}\,x\,\vxs$ though.

The types $\Size$ and $\SizeLt\,i$ are Agda primitives that are used
for termination checking.  For instance, consider the mapping function
on lists.  Note that the notation $\forall\{i\, A\, B\} \to \dots$ is
short for $\{i : \_\} \{A : \_\} \{B : \_\} \to \dots$ and becomes
$\{i : \Size\} \{A : \Set\} \{B : \Set\} \to \dots$ after type
reconstruction.

\amap

\noindent
Function $\tmap\,f$ takes a list of size $i$ as input and returns a list with the same
upper bound.  We say $\tmap$ is \emph{size preserving}.  As $\tmap$ is
defined recursively, termination is not obvious.  Agda infers from
pattern $x \tcons \vxs : \List\,i\,A$, which is short for pattern
$\tconsu\,\{j\}\,x\,\vxs : \List\,i\,A$ with a pattern variable $j$
introduced by Agda, that $\vxs : \List\,j\,A$ and $j : \SizeLt i$.
Consequently, the recursive call $\tmap\,f\,\vxs$---which internally
expands to $\tmap\,\{j\}\,f\,\vxs$--- is justified, by the descent in
size $j < i$.  An analogous argument assures termination of $\tfoldr$,
the iteration principle for lists, which replaces in a list the $\tnil$
constructor by $n : B$ and any $\tconsu$ constructor by $c : A \to B
\to B$.

\afoldr

\noindent
As an application of $\tfoldr$ and $\tmap$, we define $\tany\,p\,\vxs$
which is $\ttrue$ if $p\,x$ is $\ttrue$ for any element $x$ of $\vxs$.

\aany

Finally, data types with a single constructor can alternatively be defined as
record types.  For instance, the cartesian product $A \ttimes B$ can be
implemented as record type with the projections
% $\tfst : \forall\{A\,\B\} \to A \ttimes B \to A
$\tfst : A \ttimes B \to A$ and
$\tsnd : A \ttimes B \to B$ and
constructor $\AgdaInductiveConstructor{\_,\_} : A \to B \to A \ttimes B$.

\aprod

Agda allows the projections also on the left hand sides of
definitions by pattern matching.  In the following, we define for a
pair $p : A \ttimes B$ is reversal $\tswap\,p : B \ttimes A$ by giving
its value for all valid projections.  This definition form is called
\emph{definition by copattern matching}, since we are not matching on
a function argument, but on the possible observations on the function
result \citep{abelPientkaThibodeauSetzer:popl13}.

\aswap

\noindent
This is, of course, just one possible implementation of $\tswap$,
which we chose to exemplify copattern matching.  A simple clause
$\tswap\,(a\,,\,b) = (b\,,\,a)$ would have done the job, but copattern
matching will be the definition principle of choice for coinductive
structures in Section~\ref{sec:lang}.

\subsection{Agda as a proof assistant}

Martin-L\"of Type Theory allows us to reason about program via the
propositions-as-types paradigm.  A proposition is seen as the type of
its proofs, for instance, the absurd proposition $\tbot$
\emph{(Falsehood)} has no proof,
and the trivial proposition $\ttop$ \emph{(Truth)} has a proof with no
further content.

In Agda, $\tbot$ is modeled as a data type with no constructors.
Given a proof $p : \tbot$, we can prove any proposition $A$ (populate
any type $A$) by matching on $p$.  As there are no constructors of
$\tbot$, there is nothing further to show, indicated in Agda by the
absurd pattern $()$ which matches anything of empty type.

\abot

Truth $\ttop$ is modelled as record type with no fields.  There is no
information to extract from a proof of $\ttop$, a proof is simply an
empty record.

\atriv

Implication $A \to B$ coincides with the ordinary function space, and
universal quantification $\forall x \to A$ with the dependent
function space $(x : \_) \to A$.

We can define our own proposition, predicates, and relation as
inductive (or coinductive, see Section~\ref{sec:fameq}) families.  The
prime example is \emph{propositional equality} $x \mathrel{\tequiv} y$ of
objects $x,y : A$, which is defined as a data type with a hidden
parameter $A : \Set$, a visible parameter $x : A$, and an index $y :
A$.  The only constructor $\trefl$---which has no arguments---fixes
$y$ to be identical to $x$, thus, witnesses that $x$ and $y$ are
identical modulo Agda's internal notion of equality (which is called
\emph{definitional equality}).

\aeq

\noindent
Since defined inductively, propositional equality is the smallest
relation on $A$ which is reflexive.

Proof of equality can be used by
pattern matching.  For instance, we can prove symmetry of equality,
\ie, $x \mathrel{\tequiv} y$ implies $ y \mathrel{\tequiv} x$ by
pattern matching on the the proof of $x \mathrel{\tequiv} y$.

\aeqsym

\noindent
The only matching constructor $\trefl$ forces $y$ to be identical to
$x$.  This is indicated in Agda by the inaccessible pattern $.x$ which
means that $y$ has been instantiated by the term $x$.  As a
consequence, the goal becomes $x \mathrel{\tequiv} x$ which is simply
proved by $\trefl$.

In a similar fashion, we prove transitivity of propositional
equality.  By matching both the proofs of $x \mathrel{\tequiv} y$ and $y
\mathrel{\tequiv} z$ against $\trefl$, variables $y$ and $z$ become
instantiated to $x$, and again, the goal becomes simply
$x \mathrel{\tequiv} x$.

\aeqtrans

In general, inductively defined propositions are inhabited by proof
trees in the same way that inductive types are inhabited by trees.
% In Martin-L\"of Type Theory, these are the same thing.
As an example, consider the proposition $\Any\,i\,P\,\vxs$ which
states that predicate $P : A \to \Set$ holds on some element $x : A$
of (unbounded) list $\vxs : \List\,\tinfty\,A$.
Parameter $i : \Size$ is an upper bound
on the tree height of a proof $p : \Any\,i\,P\,\vxs$ of this
proposition.

\aAny

Constructor $\tHere$ establishes the proposition for a non-empty list
$x \tcons \vxs$ given a proof $p : P x$ that predicate $P$ holds on
the head $x$ of the list.  The resulting proof tree is a leaf, and any
size $i$ is an upper bound on the height of this derivation.  Constructor
$\tThere$ takes a derivation $p : \Any\,j\,P\,\vxs$ stating that $P$ holds on
some element of list $\vxs$, and builds a proof of
$\Any\,i\,P\,(x \tcons \vxs)$.  The tree height of $p$, ordinal $j$,
is necessarily strictly smaller than $i$.

Proofs in Type Theory naturally contain the necessary information to
construct witnesses for existential propositions such as $\Any$.  In
this case, a proof takes the form $\tThere^n\,(\tHere\,p)$ where $n$
is the index of witnessing element $x$ that satisfies $P$, and $p : P\,x$ is
the evidence for the latter fact.

This concludes the short tutorial on Type Theory and Agda.  In the
next section, we introduce coinductive types for the example of
infinitely deep trees.  In the following, we will sometimes write
$\List\,A$ for unbounded lists $\List\,\infty\,A$.

\section{Decidable Languages, Coinductively}
\label{sec:lang}

Given an alphabet $A$, a word $\vas \in \List\,A$ is a list of
characters.  A language over $A$ is usually described as set of words,
and a decidable language is one whose characteristic function
$\List\,A \to \Bool$
is computable.  We will work in the setting of Type Theory
\citep{martinlof:predicative75}
where each function is computable, thus, we can identify decidable
languages with functions of type $\List\,A \to \Bool$ where $\Bool$ is
the two-element data type with constructors $\ttrue$ and $\tfalse$.

% A potentially infinite
A set of words with decidable membership can also be
represented as \emph{trie}.  For our purposes,
a trie is an $A$-branching tree whose nodes are labelled by booleans.
Any word $\vas$ is a path into the tree selecting a subtree.  The root label
of that subtree indicates the status of the word $\vas$.  Label $\ttrue$
means the word is member of the set, label $\tfalse$ means it is not a
member.  Even though each word is finite, the language might be
infinite, thus, tries have infinite depth in general.

For instance, let us consider the language $E$ of even natural numbers in
binary representation.  Writing $0$ as $a$ and $1$ as $b$, our
language contains the words $a$, $ba$, $baa$, $bba$, $baaa$, $baba$, etc.
Given the alphabet $A = \{a,b\}$, the language can be concisely described by the
regular expression $a + b(a + b)^*a$.

\begin{figure}[htbp]
  \centering
\[
%  \entrymodifiers={++[o][F-]}
%  \SelectTips{cm}{}
  \xymatrix@C=6.6ex@R=0ex{
            &              &              &              & \cdots \\
            &              &              & *++[o][F-]{} \ar[ru]^a \ar[rd]^b&        \\
            &       & *++[o][F-]{} \ar[ru]^a \ar[rd]^b & & \cdots \\
            &              &              & *++[o][F-]{} \ar[ru]^a \ar[rd]^b&        \\
            & *++[o][F=]{} \ar[ruu]^a \ar[rdd]^b & & &     \cdots \\
            &              &              & *++[o][F-]{} \ar[ru]^a \ar[rd]^b&        \\
            &       & *++[o][F-]{} \ar[ru]^a \ar[rd]^b & & \cdots \\
            &              &              & *++[o][F-]{} \ar[ru]^a \ar[rd]^b&        \\
*++[o][F-]{} \ar[ruuuu]^a \ar[rdddd]^b & & & &             \cdots \\
            &              &              & *++[o][F=]{} \ar[ru]^a \ar[rd]^b&        \\
            &       & *++[o][F=]{} \ar[ru]^a \ar[rd]^b & & \cdots \\
            &              &              & *++[o][F-]{} \ar[ru]^a \ar[rd]^b&        \\
            & *++[o][F-]{} \ar[ruu]^a \ar[rdd]^b & & &     \cdots \\
            &              &              & *++[o][F=]{} \ar[ru]^a \ar[rd]^b&        \\
            &       & *++[o][F-]{} \ar[ru]^a \ar[rd]^b & & \cdots  \\
            &              &              & *++[o][F-]{} \ar[ru]^a \ar[rd]^b&        \\
            &              &              &              & \cdots \\
}
\]
  \caption{Trie of $E$}
  \label{fig:trie}
\end{figure}

Figure~\ref{fig:trie} shows an initial part of the trie of language
$E$, where double-circled nodes denote membership of the word leading
to that node, and single-circled ones non-membership.
Observe that the subtree $ba$ has only accepting nodes
along its left-most path $a^*$, witnessing that $ba^*$ is a
sublanguage of $E$ (representing $2^n$ for $n \geq 1$).
Thus, a finite trie would be insufficient to represent $E$.

Generalizing $\Bool$ to $B$,
the connection between
the type $T$ of $A$-branching $B$-labelled tries
and the type $\List A \to B$ can also be derived by calculating type
isomorphisms \citep{hinze:ggtries,altenkirch:tlca01}:
\[
\begin{array}{lcl}
  \List A \to B
     & \cong & (1 + A \times \List A) \to B
\\   & \cong & (1 \to B) \times ((A \times \List A) \to B)
\\   & \cong & B \times (A \to (\List A \to B))
\end{array}
% \begin{array}{lcl}
%   \List A \to B
%      & =     & (\mu X.\,1 + A \times X) \to B
% \\   & \cong & \nu Y. (1 + A \times Y) \to B
% \\   & \cong & \nu Y. (1 \to B) \times ((A \times Y) \to B)
% \\   & \cong & \nu Y. B \times (A \to Y) \to B)
% \end{array}
\]
This means that $\List A \to B$ is a
solution of the recursive equation
\[
  X \cong B \times (A \to X)
\]
which also describes the decomposition of a trie $X$ into its root label of
type $B$ and its $A$-indexed family of subtrees $A \to X$.
Tries $T$ are the greatest solution of this equation and we write
$T = \nu X.\, B \times (A \to X)$.  We will later establish the
isomorphism between $T$ and $\List A \to B$ more precisely.

% It is
% isomorphic to the $B$-valued functions over $\List A = \mu X.\,1 + A
% \times X$, as can be seen
% by a simple calculation:
% \[
% \begin{array}{lcl}
%   \List A \to B
%      & \cong & (1 + A \times \List A) \to B
% \\   & \cong & (1 \to B) \times ((A \times \List A) \to B)
% \\   & \cong & B \times (A \to (\List A \to B))
% \end{array}
% % \begin{array}{lcl}
% %   \List A \to B
% %      & =     & (\mu X.\,1 + A \times X) \to B
% % \\   & \cong & \nu Y. (1 + A \times Y) \to B
% % \\   & \cong & \nu Y. (1 \to B) \times ((A \times Y) \to B)
% % \\   & \cong & \nu Y. B \times (A \to Y) \to B)
% % \end{array}
% \]
% Since $\List A \to B$ solve

\subsection{Coinductive tries in Agda}
\label{sec:trieAgda}

\input{latex/Language}

In Agda, we represent the coinductive type
$\nu X.\, \Bool \times (A \to X)$ of tries
as a coinductive record type $\Lang$
with fields $\tnu : \Bool$ for the root label
and $\tdelta : A \to \Lang$ for the family of subtrees.
If field $\tnu$ is $\ttrue$ then the language contains the empty word,
which is sometimes called a \emph{nullable} language, hence the field
name $\tnu$.

The name $\tdelta$ is inspired by its role as
\emph{Brzozowski derivative}.
Given a decidable language $f : \List A \to \Bool$, its $a$-derivative
$\delta\,f\,a : \List A \to \Bool$ is defined as
$(\delta\,f\,a)(\vas) = f\, (a \tcons \vas)$.  This means that
$\delta\,f\,a$ accepts the words of $f$ that start with $a$, minus this
first letter.  In terms of tries $t$, we obtain the derivative
$\tdelta\,t\,a$ simply by following the $a$-labelled edge from the
root, thus, the derivation function is identical with the field
$\tdelta$.

There is one final twist to arrive at the Agda definition: In order to
facilitate corecursive definitions of tries that are certified by
Agda's productivity checker, we equip the type of tries $\Lang\,i$
with an index $i : \Size$.  Index $i$ denotes an ordinal $\leq \omega$
corresponding to the \emph{definedness depth} of a trie
$t : \Lang\,i$.  Ultimately, we are interested only in fully defined
tries $t : \Lang\,\tinfty$, where $\tinfty$ is syntax for ordinal
$\omega$.  This means we can query $t$'s nodes at arbitrary
depth.  For a finite definedness level $i$ we can only inspect
nodes up to depth $i$.  In particular $t : \Lang\,0$ allows us to look
only at the root label $\tnu\,t$, its subtrees via $\tdelta\,t\,a$ are
undefined and Agda's type checker will object to such an expression.

If for an arbitrary ordinal $i$,
a trie $t : \Lang\,i$ can be defined by reference to tries of type
$\Lang\,j$ for $j<i$, written $j : \SizeLt i$, then
$t$ can be assigned type $\forall\{i\} \to \Lang\,i$.
We say $t$ is defined by well-founded recursion on ordinal $i$,
which is our principle of corecursive definition.

\aLang

\noindent
As typing of the projections from tries we get
\[
\begin{array}{lll}
\tnu & : \forall\{i : \Size \} \to \Lang\,i \to \Bool \\
\tdelta & : \forall\{i : \Size\} \to \Lang\,i \to \forall\{j : \SizeLt i\} \to
          A \to \Lang\,j \\

\end{array}
\]
with hidden size arguments which will, if type checking succeeds, be
figured out by Agda's unifier and size constraint solver.
When taking the derivative $\tdelta\,\{i\}\,t\,\{j\}\,a$ we are free
to choose any $j$ strictly below $i$.  This expresses that if $t$ was
only defined up to depth $i$, then its $a$-subtree is less
defined; and we are allowed to waste information and chose a smaller
$j$ than necessary.  Wasting is fine since $\Lang\,i$ is a subtype of
$\Lang\,j$ whenever $i \geq j$, as we can always use a \emph{more} defined
value $t : \Lang\,i$ when a value of $\Lang\,j$ is demanded.  The
antitone subtyping chain
\[
  \Lang\,\tinfty \leq \dots \leq \Lang\,(\uparrow i) \leq \Lang\,i \leq
  \dots \Lang\,0
\]
can be justified by the equation
$\Lang\,i \cong \Bool \times \bigcap_{j<i} (A \to \Lang\,j)$
which holds semantically.

The isomorphism $\Lang\,i \cong \List\,i\,A \to \Bool$ is witnessed by
the following two functions. The first, $l \tni \vas$, checks membership
of word $\vas$ in language $l$ represented as a trie.  The empty word
$\tnil$ is in the language if $\tnu\,l$, \ie, if the language is
nullable.  The composite
word $a \tcons \vas$ is accepted by $l$ if $\vas$ is accepted by the
derivative $\tdelta\,l\,a$.
\ani{}%
Visually spoken, $l \tni \vas$ returns the root label of the subtree
selected by path $\vas$.

The second function constructs a trie representation $\ttrie\,f$ from
the functional representation $f$ of a decidable language.  The trie
is constructed corecursively by copattern matching
\citep{abelPientkaThibodeauSetzer:popl13}.
\alang{}%
The root label $\tnu\,(\ttrie\,f)$
is determined by whether $f$ accepts the empty word $\tnil$.
The $a$-derivative $\tdelta\,(\ttrie\,f)\,a$ is constructed by
corecursion on the $a$-derivative of $f$.  The justification of the
recursive call to $\ttrie$ is apparent once we make the hidden size
arguments visible:
\[
  \tdelta\;\{i\}\;(\ttrie\,\{i\}\,f)\;\{j\}\;a = \ttrie\;\{j\}\;\lambda \vas \to f\,(a \tcons \vas)
\]
By typing of the projection $\tdelta$, we have $j : \SizeLt i$, thus,
the definition of $\ttrie\,\{i\}$ only rests on $\ttrie\,\{j\}$ with a
smaller size index.  Well-founded induction on sizes guarantees that
the equation system has a unique solution.

The corecursive definition by copattern matching is sometimes likened
to differential equations \citep{hansenKupkeRutten:sde2016}.  In the definition of
$\ttrie$, the second equation ($\tdelta$) is the differential equation, and the
first equation ($\tnu$) determines the initial value.

\subsection{Constructing decidable languages by coiteration}
\label{sec:coit}

In the following, we implement some standard constructions on formal
languages by copattern matching.  These operations will allow us to
compute the trie of any regular expression.

The empty language $\tempty$ is the trie where each node label is
$\tfalse$.  Naturally, each subtree of $\tempty$ is again $\tempty$.
\aempty%
The language $\teps$ accepting only the empty word has root label
$\ttrue$ but all other labels are $\tfalse$.  Hence, any derivative is
the empty language.
\aeps%
The language $\tchar\;a$ accepting the one-letter word $a \tcons \tnil$ is
not nullable, its $a$-derivative is $\teps$ and all other derivatives
are $\tempty$.
\achar%
We obtain the language complement $\tcompl\,l$ of a language $l$ by
flipping all labels.  This is accomplished by recursing over the whole
tree.
\acompl%
Complement $\tcompl$ is a special instance of mapping a function
pointwise over all tree labels.

For the union $k \cup l$ of two languages $l$ and $k$, we overlay the
two tries and perform the Boolean disjunction operation on
corresponding node labels.
\aunion%
The intersection could be defined
analogously, using Boolean conjunction.  Both operations are instance
of a general $\tzipWith$-function that applies a binary operation
pointwise to a pair of tries.

All recursive definitions of tries so far have followed a specific pattern:
in the right hand sides of the recursive equations, the recursive call
was outermost, \ie, the equation had the form $\tdelta\,(g\,\vec y)\,x
= g\,\vec t$ for some variables $x,\vec y$ and some terms $\vec t$.
With the non-recursive equation being $\tnu\,(g\,\vec y) = o$,
this form is an instance of the commutative diagram for
terminal coalgebras and sometimes called \emph{coiteration}
\citep{geuvers:indtypes}.

For a functor $F : \Set \to \Set$, an $F$-coalgebra is a pair $(S,t)$
with $S : \Set$ and $t : S \to F\,S$.  An $F$-coalgebra morphism
between coalgebras $(S_1,t_1)$ and $(S_2,t_2)$ is a function
$f : S_1 \to S_2$ such that $t_2\,(f\, s_1)$ is equal to
$F\,f\,(t\,s_1)$ for all $s_1 : S_1$.  An $F$-coalgebra is terminal
if it is the target of a coalgebra morphism from every $F$-coalgebra.
Besides establishing the connection between coiteration and
coalgebras, we will not dwell on coalgebras in this article, thus, we
do not go into more details here.

Here is the diagram for a $(\Bool \times (A \to \_))$-coalgebra
$(\Gamma, h)$ mapping into the terminal coalgebra $(\Lang, \langle \tnu, \tdelta \rangle)$:
\[
% \newcommand{\tnd}{\langle o,\; \lambda x \to \vec t \rangle}
\newcommand{\tnd}{\,h}
\xymatrix@R=12ex@C=12ex{
  \Gamma \ar[r]^(0.45)*{\tnd} \ar@{.>}[d]_*{%g\,\vec y \,:=\,
    \tcoit\tnd}
    & *!<-1.6ex,0ex>{~\Bool \times (A \to \Gamma)} \ar[d]^*{\tid \times (\tcoit\tnd\, \circ \_)}\\
  \Lang \ar[r]^(0.45)*{\langle \tnu, \tdelta \rangle}
    & *!<-3.2ex,0ex>{~\Bool \times (A \to \Lang)} \\
}
\]
With $g := \tcoit\,h$, the commutative law
% $\langle \tnu, \tdelta \rangle \circ \tcoit\,h = \tid \times (\tcoit\,h\,
% \circ \_) \circ h$.
\[\langle \tnu, \tdelta \rangle \circ g = \tid \times (g \circ \_) \circ h\]
can be applied to points $\vec y : \Gamma$ to yield
\[\langle \tnu, \tdelta \rangle (g\,\vec y)
  = (\tid \times (g \circ \_)) (h\,\vec y).\]
For our instance, $h\, \vec y= (o,\, \lambda x \to \vec t)$
with $\vec y \of \Gamma \der o : \Bool$ and $\vec y \of \Gamma, x \of A \der
t : \Lang$, thus,
\[\langle \tnu, \tdelta \rangle (g\,\vec y)
  = (o,\; g \circ (\lambda x \to \vec t)).\]
This can be split into the two equations
\[
\begin{array}{lll}
  \tnu\,(g\,\vec y) & = & o \\
  \tdelta\,(g\,\vec y)\,x & = & g\,\vec t \\
\end{array}
\]
that form the laws of a function
$g = \tcoit\;(\lambda \vec y \to (o,\, \lambda x \to \vec t))$
defined by coiteration (modulo some tupling and (un)currying).

The type/context $\Gamma$ can be interpreted as the set of states of
an automaton $h$ with a coupled presentation of the accepting state
set $\Gamma \to \Bool$ and the transition function $\Gamma \to (A \to \Gamma)$.
Function $\tcoit\,h$ maps a state $s : \Gamma$ to the language
$\tcoit\,h\,s$ accepted by $h$ starting from state $s$.
The discussed language constructions correspond to
constructions of (possibly infinite) automata
with references to existing automata as oracles. The
reader is invited to confirm this by expressing the given
constructions through coiteration.  Note however, that the state type $\Gamma$
might involve $\Lang$ and is, thus, not guaranteed to be finite!


\subsection{Constructing decidable languages by well-founded corecursion}

To complete the constructions of languages as supported by regular
expressions, we are missing language concatenation and the Kleene
star.  These can be constructed by \emph{corecursion up-to} which can
be reduced to primitive corecursion into a trie with an extended
alphabet \citep{traytel:fscd16}.  However, using sized types we can
naturally define these operations by their derivative laws, using
well-founded recursion on sizes.

Language concatenation $k \tdot l$ is our first non-trivial operation on languages.
The intuition
$(k \tni \vas) \wedge (l \tni \vbs) \imp (k \tdot l) \tni (\vas \oapp \vbs)$
leads to the specification
$(k \tdot l) \tni \vcs
  \iff \exists n \in \NN.\; k \tni (\ttake\,n\,\vcs) \wedge l \tni (\tdrop\,n\,\vcs)
$.\footnote{
We write $\vas \oapp \vbs$ for the concatenation of lists $\vas$ and
$\vbs$; we write $\ttake\,n\,\vcs$ for the largest prefix of $\vcs$ of
length $\leq n$, and $\tdrop\,n\,\vcs$ for the remainder. Note that
$\vcs = \ttake\,n\,\vcs \oapp \tdrop\,n\,\vcs$ for any $n \in \NN$.
}
However, this specification
does not directly suggest a pretty implementation of $k \tdot l$
\citep{doczkalKaiserSmolka:cpp13}.

We can instead try to understand language concatenation as an operation on the
tries $k$ and $l$.  If we think about accepting a word $\vcs$ in $k \tdot l$
by following pathes in $k$ and $l$, the following procedure applies:
We start by following branches in $k$.  Whenever we reach an accepting
node in $k$ we may decide that we have reached the boundary between
the words $\vas$ in $k$ and $\vbs$ in $l$ that make up the word $\vcs
= (\vas \oapp \vbs)$ in $k \tdot l$. Hence, we start following branches in
$l$.  However, since we are not sure we already reached the boundary,
we simultaneously continue to follow branches in $k$.  At each
accepting node in $k$ we spawn off a run in $l$.  Thus, a trie for $k
\tdot l$ may be constructed by the following operation on all
accepting nodes of $k$:  make the node non-accepting but then union the
subtree starting here with $l$.  This transformation is achieved by
the following corecursive definition of concatenation:
\aconcat
The concatenation of two languages is nullable iff both are nullable.
For the $x$-derivative, we follow the $x$-branch in $k$
via $\tdelta\,k\,x \tdot l$ in any case.  If the node is accepting,
\ie, $\tnu\,k$ is $\ttrue$,
we may in addition follow the $x$-branch in $l$ via $\tdelta\,l\,x$.
As before, the equations for language concatenation correspond to the
derivation laws of regular expressions \citep{brzozowski:jacm64},
but we arrived there by the trie intuition.

The above definition is not an instance of coiteration for two
reasons:  First, the outermost call is to $\tifthenelse$ rather than
the recursive call $\kpl$.  Even if we consider $\tifthenelse$ to be special
(rather than just an arbitary Agda function), there is still a
recursive call $\kpl$ in the then-branch which is not at top-level,
but under the union-operator.  This
problem is usually fixed by defining a scheme for corecursion up to
union.  However, looking at the involved sizes we can accept the
definition in the present form as an instance of well-founded corecursion.
Crucial here is the sized typing of the union
\[
  \_\tcup\_ : \forall\{i\} (k\, l : \Lang\,i) \to \Lang\,i
\]
which asserts that the arguments are no deeper analyzed than the
definedness depth of the result.  If we make all hidden size arguments
visible---having to switch to prefix operators instead of infix ones---%
we can see the propagation of definedness depth levels to the
recursive call $\kpl$.
\aconcatexpl%
Since the recursive call happens at smaller index $j < i$, it is justified.
Note also that in the definition of $\kpl$, last letter, $l : \Lang\,i$ is
cast to $\Lang\,j$ which is valid since $j < i$.

The iteration $l^*$ of a language $l$, aka \emph{Kleene star}, can be
informally described as ``zero or more repetitions of $l$''.  If for
some $n \geq 0$ we have words $\vas_1, \vas_2, \dots \vas_n \in l$, then
$(\vas_1 \oapp \vas_2 \oapp \dots \oapp \vas_n) \in l$.  In terms of tries, $l^*$
is obtained from $l$ by making the root accepting and unioning $l$
with any
subtree of $l$ that has an accepting root.  Intuitively, this means that at each
accepting node we may ``jump back'' to the root.  The corecursive definition
\astar%
relies on the sized typing of concatenation to justify the recursive call.

This concludes our set of language operations defined by well-founded
corecursion.   These operations allow us to give an executable
semantics for regular expressions (leaving aside efficiency questions).
It may be remarked that, thanks to sized typing, all the definitions are concise
and direct counterparts of the derivative laws for regular expressions
\citep{brzozowski:jacm64}.

\section{Proving the Kleene Algebra Laws}
\label{sec:kleene}

In this section, we prove that decidable languages as introduced in
Section~\ref{sec:lang} form a \emph{Kleene algebra}.

\subsection{A family of equivalence relations over languages}
\label{sec:fameq}

Equality of tries, sometimes called \emph{strong bisimilarity}, is
defined coinductively as follows.  Two tries are strongly bisimilar if
they have the same root and corresponding subtries are strongly
bisimilar in turn.  In Agda, this amounts to the following coinductive
definition:
\abisim
For one, note that we are relating tries $l,k : \Lang\,\tinfty$ whose
definedness depth is unbounded ($\tinfty$).  This means that any
subtrie such as $\tdelta\,l\,a$ is defined and in turn has type
$\Lang\,\tinfty$.

However, the relation itself is indexed by a definedness depth $i$.
In fact we are defining a family of types such that $\bisim l j k$
is a subtype of $\bisim l i k$ whenever $i \leq j$.  The depth is a
lower bound on how far the proof of equality of $l$ and $k$ is
constructed.  In particular, we can only inspect the derivative
$\cdelta\,p\,a$ of a proof $p : \bisim l i k$ if $i > 0$.  As for
coinductive types like $\Lang\,i$, the size index $i$ is just a tool
for the corecursive construction of derivations.  Ultimately, we are
only interested in fully defined equality proofs $p : \bisim l \tinfty k$.
In particular, our size-index relation is not to be confused with \emph{ordered families
  of equivalences} (OFEs) \citep{gianantonioMiculan:types02} $l \equiv_n k$
which refine the notion of equality itself.  (There, $l \equiv_0 k$
would hold always and $l \equiv_{n+1} k$ would hold if $l$ and $k$
have equal roots and their immediate subtries are $\equiv_n$-related.)
OFEs are a different approach to justifying corecursive definitions.

Each of the coinductive relations forms an equivalence relation,
proven for the whole family by coiteration.  For reflexivity, we have
to prove that given a trie $l$, we can construct a derivation that $l$
is strongly bisimilar to itself $l$, up to arbitrary depth $i$.
\arefl
The proof $\tcongrefl$ of $\bisim l i l$ is constructed lazily.  If we
are asking for its first component $\congnu\,\tcongrefl$ we get a proof
that the root $\tnu\,l$ is identical to itself, namely $\trefl :
\tnu\,l \,\tequiv\, \tnu\,l$.  If we are asking for the $a$-branch of
its second component, $\congdelta\,\tcongrefl\,a$ at depth $j < i$, it
computes $\tcongrefl : \bisim l j l$ corecursively.

Symmetry is defined in a similar fashion.  To compute a proof of $k \cong l$
up to depth $i$, we only need a derivation of $l \cong k$ up to depth
$i$; thus, the type of $\tcongsym$ is $\bisim l i k \to \bisim k i l$.
\asym
Transitivity is likewise depth preserving.  Depth-preservation is
crucial to combine reasoning by transitivity and the coinductive
hypothesis in a natural way, as we will see below.
\atrans
Taken together, each $\bisim \_ i \_$ is an equivalence relation, and
forms a setoid $\Bis\,i$ with carrier $\Lang\,\tinfty$.
\asetoid
Later, we will use these setoids to reason by equality chains.

\subsection{Laws of language union}
\label{sec:lawunion}

Decidable languages form an idempotent commutative monoid under
union.  The individual laws, like associativity, commutativity,
idempotency, and unit, follow from the corresponding laws of the
boolean disjunction, which are pointwise applied at all the
corresponding nodes of the involved tries.  In Agda, these are direct
proofs by coiteration.
\aunionassoc
\aunioncomm
\aunionidem
\aunionemptyl
% \aunionemptyr
Finally, union preserves equality, which is again proven by
coiteration.
The sized typing will be crucial to apply a coinductive hypothesis
under $\tunioncong$ later.
\aunioncong
A derived law we require later is that union distributes over itself.
Now that we have established that union fullfills the laws of an
idempotent commutative monoid, we can use a solver to prove this law
automatically by reflection.
\aunionuniondistr
Concretely, the solver checks that both sides of the equation have the
same set of atoms, by normalizing both sides to the set $\{k,l,m\}$.
This solver is implemented in Agda itself, but we will not describe it
further here.\footnote{%Implementation see
\url{https://github.com/agda/agda-stdlib/blob/1c78e4e/src/Algebra/IdempotentCommutativeMonoidSolver.agda}
implements this solver.
}

\subsection{Laws of language concatenation}
\label{sec:lawcat}

Concatenation distributes over union, for instance,
$k \tdot (l \tunion m) \cong (k \tdot l) \tunion (k \tdot m)$.
Naturally, we would like to prove this by coinduction.  The case for
$\tnu$ follows by the boolean distributivity law
$x \wedge (y \vee z) = (x \wedge y) \vee (x \wedge z)$.  In the case
for $\tdelta$, we would like to reason by the following equality
chain.  We consider the subcase that $k$ is nullable, and underline
the subterms that have changed from the last line (unless the whole
expression has changed).
\[
\def\arraystretch{1.5}
\begin{array}{cl@{\qquad}r}
\derive{(k \tdot (l \tunion m))} a
  & \cong & \mbox{by definition} \\
\derive k a \tdot (l \tunion m) \tunion {\derive{(l \tunion m)} a}
  & \cong & \mbox{by definition} \\
{\derive k a \tdot (l \tunion m)} \tunion \underline{(\derive l a \tunion \derive m a)}
  & \cong & \mbox{by coinduction hypothesis} \\
\underline{(\derive k a \tdot l \tunion {\derive k a \tdot m})} \tunion ({\derive l a} \tunion \derive m a)
  & \cong & \mbox{by union laws} \\
{(\derive k a \tdot l \tunion \underline{\derive l a})} \tunion {(\underline{\derive k a \tdot m} \tunion \derive m a)}
  & \cong & \mbox{by definition} \\
\underline{\derive{(k \tdot l)} a} \tunion \underline{\derive{(k \tdot m)} a}
  & \cong & \mbox{by definition} \\
\derive{(k \tdot l \tunion k \tdot m)} a
\end{array}
\]
This proof does not follow the scheme of (primitive) coinduction. The
coinduction hypothesis is applied under uses of transitivity (for
connecting the equations) and under the congruence law for union.
This becomes especially clear if we fully write out the
justifications as in the corresponding Agda proof in
Figure~\ref{fig:concatuniondistribr}.  However, the continuity of
transitivity and $\tunioncongl$ as witnessed by the sized typing
justifies the use of the coinduction hypothesis.

% \begin{figure}[htbp]
%   \centering
% %\hrule width0.8\textwidth
% \aconcatuniondistribl
%   \caption{Concatenation distributes over union.}
%   \label{fig:concatuniondistribl}
% \end{figure}

\begin{figure}[htbp]
  \centering
%\hrule width0.8\textwidth
\aconcatuniondistribr
  \caption{Concatenation distributes over union.}
  \label{fig:concatuniondistribr}
\end{figure}

The other distributivity law is proven by coinduction and case
distinction over the nullability of $l$ and $k$.
\aconcatuniondistribl
Congruence laws for concatenation follow by coinduction and congruence
of union.
\aconcatcongl
\aconcatcongr
The coinductive proof of associativity relies on distributivity and
congruence and associativity of union.
\aconcatassoc
Finally, the empty language is a zero and the language of the empty
word a unit for language composition:
\aconcatemptyl
\aconcatemptyr
\aconcatunitl
\aconcatunitr

\subsection{Laws of the Kleene star}
\label{sec:lawstar}

The language of the empty word is the iteration of the empty language.

\astarempty

\noindent
To prove that iteration is idempotent, we first prove that
concatenation of iterated languages is idempotent.

\astarconcatidem

\noindent
This lets us prove idempotency of the Kleene star:

\astaridem

\noindent
The Kleene star obeys the following recursive equation:

\astarrec

\noindent
Finally, we prove Arden's rule \citeyearpar{arden:focs61},
which would allow us to solve linear
equations over regular expressions.
%\astarconcat
\astarfromrec

All the proofs about decidable languages in this section
could be carried out rather mechanically using:
\begin{enumerate}\setlength{\itemsep}{0ex}
\item coinduction,
\item equality chains,
\item already proven lemmata.
\end{enumerate}
We did not require any up-to techniques or creative insight such as
finding bisimulation relations to carry out our proofs.
% that is closed under $\tcongnu$ and $\tcongdelta$.
Thus, it is likely that after initiating coinduction, standard
first-order theorem provers could fill in the remaining steps.


\section{Constructing Automata}
\label{sec:aut}

\input{latex/Automaton}

In this section, we show that deterministic automata form a Kleene
algebra like decidable languages do.  We show how to construct union,
concatenation, and Kleene star of automata, in a recapitulation of
the classic theory of formal languages.  Our message is that the
corresponding correctness proofs can be carried out by the same means
as in the last section: coinduction and equational reasoning.

In out presentation of deterministic automata (DA) we follow
\cite{rutten:concur98}:  A not necessarily finite automaton over a
state set $S$ is given by a transition function
$\tdelta : S \to A \to S$ and a characteristic function
$\tnu : S \to \Bool$ for the
set of accepting (or final) states.  These two functions could also be
bundled as $S \to \Bool \times (A \to S)$, making the coalgebra
structure apparent.

In anticipation of power automata we lift the coalgebra to lists of
states $\List\,i\,S \to \Bool \times (A \to \List\,i\,S)$.  A lists of
states is accepting if it contains at least one final state.  And we
step to a new list of states by pointwise applying the transition function.

\aDA

The initial state is not
contained in the automaton definition; each state $s$
induces a language $\tlang\,\vda\,s$
accepted by an automaton $\vda$, which can be defined by simple coiteration:

\aacclang

\subsection{Simple constructions on automata}
\label{sec:simpleaut}

An automaton for the empty language can be constructed with a single
non-accepting state inhabiting Agda's unit type $\top$.

\begin{minipage}{0.5\linewidth}
\aemptyA
\end{minipage}
\begin{minipage}{0.5\linewidth}
\[
\xymatrix{
  \bullet \ar@(dr,ur)
}
\]
\end{minipage}

To recognize the language of the empty word, we take need two states,
accepting $\ttrue$ and non-accepting $\tfalse : \Bool$.

\begin{minipage}{0.5\linewidth}
\aepsA
\end{minipage}
\begin{minipage}{0.5\linewidth}
\[
\xymatrix{
  *++[o][F=]{\ttrue } \ar[r] &
  *++[o][F-]{\tfalse} \ar@(ur,ul)
}
\]
\end{minipage}

For accepting a the single letter word $a$, we have three states: an
initial state $\tinit$, an accepting state $\tacc$, and a rejecting
error state $\terr$.

\begin{minipage}{0.5\linewidth}
\acharA
\end{minipage}
\begin{minipage}{0.5\linewidth}
\[
\xymatrix{
  *++[o][F-]{\tinit } \ar[r]^{a} \ar[dr]_{\neg a} &
  *++[o][F=]{\tacc } \ar[d] \\
&
  *++[o][F-]{\terr} \ar@(dr,dl)
}
\]
\end{minipage}

Given an automaton $\vda$, we construct the automaton $\tcomplA\,\vda$
for the complement language by switching accepting and non-accepting states.

\acomplA

Given an automaton $\vda_1$ over state set $S_1$ accepting language $\ell_1$ and an
automaton $\vda_2$ over $S_2$ for $l_2$, we can recognize the union $\ell_1
\tunion \ell_2$ by the following \emph{product} automaton $\vda_1
\toplus \vda_2$ over state set $S_1 \times S_2$.  A state in the
product automaton is a pair of states $(s_1, s_2)$, one from each
original automaton.  Transitions are done in lock-step, and for
acceptance at least one of the original automata must be in a final
state.

\aunionA

\subsection{Automaton composition for language concatenation}
\label{sec:compaut}

In preparation for automaton constructions for language concatenation
and iteration, we define the power automaton, which allows us to be in
a set of states at the same time.  It is actually sufficient to
consider finite sets of states, which we represent a bit redundantly
as lists.

\apowA

If we start the power automaton in state $[s_1,\dots,s_n]$, the
accepted language will be the $\bigcup_{i=1}^n \tlang\,\vda\,s_i$.  We
prove this in two steps:
First, if we start out in no states, the accepted language is empty.

\apowAnil

If we start in the non-empty list $s \tcons \varss$, we accept the union of
the accepted language of $\vda$ from $s$ and the accepted language of
$\tpowA\,\vda$ from $\varss$.

\apowAcons

For language concatenation, given two automata $\vda_1$ and $\vda_2$,
we will construct a composition automaton
$\tcomposeA\,\vda_1\,s_2\,\vda_2$ such that its accepted language from
state $s_1$ is the language concatenation
$\tlang\,\vda_1\,s_1 \tdot \tlang\,\vda_2\,s_2$.
The key insight is that whenever we reach a final state $s_f$ in $\vda_1$, we
non-deterministically jump to the initial state $s_2$ of $\vda_2$.
In some formulations of non-deterministic automata this would be an
$\varepsilon$-transition from $s_f$ to $s_2$, consuming no input.
We will instead add transitions from $s_f$ to the successor states of $s_2$.
\[
\xymatrix{
\bullet & & & & \bullet \\
\vda_1 % \ar@{.>}[r]
& *++[o][F=]{s_f}
   \ar[ul]_a
   \ar[dl]^b
   \ar@{.>}[rr]^{\varepsilon}
   \ar@{.>}@/^/[rrru]^a
   \ar@{.>}@/_/[rrrd]_b
&
& *++[o][F-]{s_2} \ar[ur]^a \ar[dr]_b
& \vda_2 % \ar@{.>}[l]
\\
\bullet & & & & \bullet \\
}
\]

This means for the composition that
we are in one state of $\vda_1$ and in zero or more states
of $\vda_2$ at the same time.  Thus, the type of states is
$S_1 \times \List\,\tinfty\,S_2$ and we consider the power of the
second automaton.

\acomposeA

A state $(s_1, \varss_2)$ of the composition automation is final if any
of $\varss_2$ is final, or if $s_1$ is final and the initial state $s_2$
of $\vda_2$ is also a final state.  (The latter means that the second
language is nullable, so any word of the first language is contained
in the composition.)

\acomposeAnu

To step from state $(s_1, \varss_2)$ we consider two cases.  First, if
$s_1$ is not final, we simply transition pointwise, from $s_1$ with
$\tdelta\, \vda_1$, and from each state in $\varss_2$ with $\tdelta\,\vda_2$.
However, if $s_1$ is final, we consider us to be also in the initial
state $s_2$ of $\vda_2$, thus, we add to this the transition we can
make from $s_2$ in the second automaton.

\acomposeAdelta

The composition automaton is a non-trivial construction, thus, it
makes sense to look at its correctness proof.  We have to generalize
the correctness statement to arbitrary initial states $(s_1, \varss)$ in
the composition automaton.  If $\varss$ is not empty, the accepted
language of the composition automaton
contains the union of the accepted languages from each state
in $\varss$ as well.

\acomposeAgen

The proof is by coinduction, using lemma $\tpowAcons$ in case $s_1$ is
final.

\acomposeAgenproof

As a corollary for empty $\varss$, we obtain the correctness of automaton composition:

\acomposeAcorrect


\subsection{Automaton construction for language iteration}
\label{sec:staraut}

Finally, we construct from an automaton $\vda$ accepting language
$\ell$ from state $s_0 : S$ an
automaton $\tstarA\,\vda$ for the iterated language $\ell^*$.
We do this in two steps:
\begin{enumerate}
\item $\tacceptingInitial$: Add a new final state $\tnothing : \Maybe\,S$ with the same successors as
  $s_0$.  State $\tnothing$ will serve as the new initial state.  Its
  finality guarantees that the empty word is accepted.
\item $\tfinalToInitial$: Add the successors of $s_0$ to each final state.  This enables
  iteration.  At this point, the automaton becomes
  ``non-deterministic'', \ie, we switch to $\List\,\tinfty\,(\Maybe\,S)$.
\end{enumerate}
\[
\xymatrix{
&&&& \bullet
\\
\ar@{.>}[r]
&*++[o][F=]{\tnothing}
  \ar@{.>}@/^/[rrru]^a
  \ar@{.>}@/_/[rrd]_b
&
& *++[o][F-]{s_0}
  \ar[ur]^a
  \ar[d]_b
& \vda~~~~~
& *++[o][F=]{}
  \ar@{.>}@/_/[ul]_a
  \ar@{.>}[dll]^b
\\
&&& \bullet
&& *++[o][F=]{}
  \ar@{.>}@(ul,d)[uul]_>>>>>>{a}
  \ar@{.>}[ll]^b
\\
}
\]

\noindent
The first step embeds states $s : S$ of $\vda$ as $\tjust\,s : \Maybe\,S$.

\aaccinit

\noindent
It adds the new accepting state $\tnothing : \Maybe\,S$ with the
successors of $s_0$.

\aaccinitnothing

\noindent
The second step constructs the power automaton and add transitions
from the final states to the successors of the initial state.

\afinalinit

\noindent
Composing these steps leads to the automaton for language iteration.

\astarA

To verify the construction, we first note some properties of the first
step.  For one, embedding the states of $\vda$ via
$\tjust : S \to \Maybe\,S$ does not change the accepted language.

\aacceptingInitialjust

\noindent
This lemma is proven directly by coinduction.
Further, is accepted language by the new state $\tnothing : \Maybe\,S$
is the language accepted by $s_0$ enriched with the empty word.

\aacceptingInitialnothing

\noindent
The proof by coinduction uses $\tacceptingInitialjust$.

The main lemma characterizes the language accepted by
$\tstarA\,s_0\,\vda$ from an arbitrary state $\varss$.

\astarAlemma

The proof by coinduction uses $\tpowAcons$, $\tacceptingInitialjust$,
and some laws of decidable languages as proven in Section~\ref{sec:lang}.

Finally, we prove correctness of the $\tstarA$-construction:  If we
start in the new initial state $\tnothing$ (only), the recognized
language is the Kleene star of the language recognized by $\vda$ from $s_0$.

\astarAcorrect

The proof is direct,
instantiating the $\tstarAlemma$, using correctness of the
$\tpowA$-construction, and lemma $\tacceptingInitialnothing$.




\section{Conclusions}
\label{sec:concl}

In this article, we have demonstrated that


\bibliographystyle{elsarticle-harv}
\bibliography{auto-jlamp17}

\end{document}

\endinput
%%
%% End of file
