\nonstopmode
\documentclass[preprint,12pt,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{ccicons}
\usepackage{xspace}

\usepackage{bbm}
\usepackage[usenames,dvipsnames]{color}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=Red, citecolor=ForestGreen,
  urlcolor=RoyalBlue}
\usepackage{natbib}
\usepackage[postscript]{ucs}
% Andreas: auto-generated not needed now
% \usepackage[postscript,autogenerated]{ucs}
\usepackage{pifont}
\usepackage{textgreek}
\usepackage[utf8x]{inputenc}
\usepackage{src/latex/agda}


\DeclareUnicodeCharacter{"02E1}{\ensuremath{{}^{\mathsf{l}}}}
\DeclareUnicodeCharacter{"02E2}{\ensuremath{{}^{\mathsf{s}}}}
\DeclareUnicodeCharacter{"03B4}{\ensuremath{\delta}}
\DeclareUnicodeCharacter{"03B5}{\ensuremath{\varepsilon}}
\DeclareUnicodeCharacter{"03BB}{\ensuremath{\lambda}}
\DeclareUnicodeCharacter{"03BD}{\ensuremath{\nu}}
\DeclareUnicodeCharacter{"1D9C}{\ensuremath{{}^{\mathsf{c}}}}
%\DeclareUnicodeCharacter{"1D52}{\ensuremath{{}^{\mathrm{o}}}} % UNUSED
\DeclareUnicodeCharacter{"1D62}{\ensuremath{{}_i}}
\DeclareUnicodeCharacter{"2032}{\kern0.07em\ensuremath{'}}
\DeclareUnicodeCharacter{"2092}{\ensuremath{{}_o}}
% ALT: rm variant (Andreas: I think italics looks better)
%\DeclareUnicodeCharacter{"1D62}{\ensuremath{{}_{\mathrm{i}}}}
%\DeclareUnicodeCharacter{"2092}{\ensuremath{{}_{\mathrm{o}}}}  % or italics?
\DeclareUnicodeCharacter{"2099}{\ensuremath{{}_{n}}} % here we use italics
\DeclareUnicodeCharacter{"2200}{\ensuremath{\forall}}
\DeclareUnicodeCharacter{"2205}{\ensuremath{\emptyset}}
\DeclareUnicodeCharacter{"220B}{\ensuremath{\ni}}
\DeclareUnicodeCharacter{"220E}{\ensuremath{\scriptstyle{\blacksquare}}}
\DeclareUnicodeCharacter{"2237}{\ensuremath{::}}
\DeclareUnicodeCharacter{"2227}{\ensuremath{\wedge}}
\DeclareUnicodeCharacter{"2228}{\ensuremath{\vee}}
\DeclareUnicodeCharacter{"222A}{\ensuremath{\cup}}
\DeclareUnicodeCharacter{"25B9}{\ensuremath{\vartriangleright}}
\DeclareUnicodeCharacter{"225F}{\ensuremath{\stackrel{?}{=}}}

\usepackage[all]{xy}
\input{macros}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{JLAMP}
%\journal{Journal of Logic and Algebraic Methods in Programming}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{Equational Reasoning about Formal Languages in Coalgebraic
  Style
}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}

\author{Andreas Abel}

\address{Department of Computer Science and Engineering, Gothenburg University}


\begin{abstract}

\end{abstract}

\begin{keyword}
Coalgebra
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
\end{keyword}

\end{frontmatter}

%\tnotetext{}
\ccbyncnd{} This paper is published under a CC-BY-NC-ND license.

%% \linenumbers

\section{Introduction}
\label{sec:intro}

Formal languages and automata are a foundational topic of computer science, with
many practical applications such as compiler construction, textual search, model
checking, and decidability of certain logics.
Automata are an instance of transitions systems which have the
structure of a coalgebra.
Coalgebraic and coinductive reasoning tools such as simulations and bisimulations have
been successfully employed to study formal languages. % \cite{}.
Coinduction-up-to techniques have emerged as the state of the art to
get coinductive proofs through. % \cite{sangiorgi}.

This paper presents an anti-thesis to up-to techniques.  We show how
much of the reasoning about formal languages can be carried out already with
the coinductive notion of equality of languages aka bisimilarity.  We
formalize a coinductive type of languages and the coinductive type
family of strong bisimilarity of languages in Agda using sized types.
The sized typing enables us to do coinductive proofs of bisimilarity
by equational reasoning as customary to establish algebraic
properties.  In particular, after establishing that formal languages
form a Kleene algebra, the laws of the Kleene algebra are sufficient
to prove correctness of the usual automata constructions.


\section{Preliminaries: Type Theory and Agda}
\label{sec:prelim}

Sized inductive types: $\List$.

Hidden arguments.

\section{Decidable Languages, Coinductively}
\label{sec:lang}

Given an alphabet $A$, a word $\vas \in \List\,A$ is a list of
characters.  A language over $A$ is usually described as set of words,
and a decidable language is one whose characteristic function
$\List\,A \to \Bool$
is computable.  We will work in the setting of Type Theory
\citep{martinlof:predicative75}
where each function is computable, thus, we can identify decidable
languages with functions of type $\List\,A \to \Bool$ where $\Bool$ is
the two-element data type with constructors $\ttrue$ and $\tfalse$.

% A potentially infinite
A set of words with decidable membership can also be
represented as \emph{trie}.  For our purposes,
a trie is an $A$-branching tree whose nodes are labelled by booleans.
Any word $\vas$ is a path into the tree selecting a subtree.  The root label
of that subtree indicates the status of the word $\vas$: label $\ttrue$
means the word is member of the set, label $\tfalse$ means it is not a
member.  Even though each word is finite, the language might be
infinite, thus, tries have infinite depth in general.

For instance, let us consider the language $E$ of even natural numbers in
binary representation.  Writing $0$ as $a$ and $1$ as $b$, our
language contains the words $a$, $ba$, $baa$, $bba$, $baaa$, $baba$, etc.
Given the alphabet $A = \{a,b\}$, the language can be concisely described by the
regular expression $a + b(a + b)^*a$.

\begin{figure}[htbp]
  \centering
\[
%  \entrymodifiers={++[o][F-]}
%  \SelectTips{cm}{}
  \xymatrix@C=6.6ex@R=0ex{
            &              &              &              & \cdots \\
            &              &              & *++[o][F-]{} \ar[ru]^a \ar[rd]^b&        \\
            &       & *++[o][F-]{} \ar[ru]^a \ar[rd]^b & & \cdots \\
            &              &              & *++[o][F-]{} \ar[ru]^a \ar[rd]^b&        \\
            & *++[o][F=]{} \ar[ruu]^a \ar[rdd]^b & & &     \cdots \\
            &              &              & *++[o][F-]{} \ar[ru]^a \ar[rd]^b&        \\
            &       & *++[o][F-]{} \ar[ru]^a \ar[rd]^b & & \cdots \\
            &              &              & *++[o][F-]{} \ar[ru]^a \ar[rd]^b&        \\
*++[o][F-]{} \ar[ruuuu]^a \ar[rdddd]^b & & & &             \cdots \\
            &              &              & *++[o][F=]{} \ar[ru]^a \ar[rd]^b&        \\
            &       & *++[o][F=]{} \ar[ru]^a \ar[rd]^b & & \cdots \\
            &              &              & *++[o][F-]{} \ar[ru]^a \ar[rd]^b&        \\
            & *++[o][F-]{} \ar[ruu]^a \ar[rdd]^b & & &     \cdots \\
            &              &              & *++[o][F=]{} \ar[ru]^a \ar[rd]^b&        \\
            &       & *++[o][F-]{} \ar[ru]^a \ar[rd]^b & & \cdots  \\
            &              &              & *++[o][F-]{} \ar[ru]^a \ar[rd]^b&        \\
            &              &              &              & \cdots \\
}
\]
  \caption{Trie of $E$}
  \label{fig:trie}
\end{figure}

Figure~\ref{fig:trie} shows an initial part of the trie of language
$E$, where double-circled nodes denote membership of the word leading
to that node.  Observe that the subtree $ba$ has only accepting nodes
along its left-most path $a^*$, witnessing that $ba^*$ is a
sublanguage of $E$ (representing $2^n$ for $n \geq 1$).
A finite trie would be insufficient to represent $E$.

The connection between
the type $T$ of $A$-branching $B$-labelled tries
and the type $\List A \to B$ can also be derived by calculating type
isomorphisms:
\[
\begin{array}{lcl}
  \List A \to B
     & \cong & (1 + A \times \List A) \to B
\\   & \cong & (1 \to B) \times ((A \times \List A) \to B)
\\   & \cong & B \times (A \to (\List A \to B))
\end{array}
% \begin{array}{lcl}
%   \List A \to B
%      & =     & (\mu X.\,1 + A \times X) \to B
% \\   & \cong & \nu Y. (1 + A \times Y) \to B
% \\   & \cong & \nu Y. (1 \to B) \times ((A \times Y) \to B)
% \\   & \cong & \nu Y. B \times (A \to Y) \to B)
% \end{array}
\]
This means that $\List A \to B$ is a
solution of the recursive equation
\[
  X \cong B \times (A \to X)
\]
which also describes the decomposition of a trie $X$ into its root label of
type $B$ and its $A$-indexed family of subtrees $A \to X$.
Tries $T$ are the greatest solution of this equation and we write
$T = \nu X.\, B \times (A \to X)$.  We will later establish the
isomorphism between $T$ and $\List A \to B$ more precisely.

% It is
% isomorphic to the $B$-valued functions over $\List A = \mu X.\,1 + A
% \times X$, as can be seen
% by a simple calculation:
% \[
% \begin{array}{lcl}
%   \List A \to B
%      & \cong & (1 + A \times \List A) \to B
% \\   & \cong & (1 \to B) \times ((A \times \List A) \to B)
% \\   & \cong & B \times (A \to (\List A \to B))
% \end{array}
% % \begin{array}{lcl}
% %   \List A \to B
% %      & =     & (\mu X.\,1 + A \times X) \to B
% % \\   & \cong & \nu Y. (1 + A \times Y) \to B
% % \\   & \cong & \nu Y. (1 \to B) \times ((A \times Y) \to B)
% % \\   & \cong & \nu Y. B \times (A \to Y) \to B)
% % \end{array}
% \]
% Since $\List A \to B$ solve

\subsection{Coinductive tries in Agda}
\label{sec:trieAgda}

\input{latex/Language}

In Agda, we represent the coinductive type
$\nu X.\, \Bool \times (A \to X)$ of tries
as a coinductive record type $\Lang$
with fields $\tnu : \Bool$ for the root label
and $\tdelta : A \to \Lang$ for the family of subtrees.
If field $\tnu$ is $\ttrue$ then the language contains the empty word,
which is sometimes called a \emph{nullable} language, hence the field
name $\tnu$.

The name $\tdelta$ is inspired by its role as
\emph{Brzozowski derivative}.
Given a decidable language $f : \List A \to \Bool$, its $a$-derivative
$\delta\,f\,a : \List A \to \Bool$ is defined as
$(\delta\,f\,a)(\vas) = f\, (a :: \vas)$.  This means that
$\delta\,f\,a$ accepts the words of $f$ that start with $a$ minus this
first letter.  In terms of tries $t$, we obtain the derivative
$\tdelta\,t\,a$ simply by following the $a$-labelled edge from the
root, thus, the derivation function is identical with the field
$\tdelta$.

There is one final twist to arrive at the Agda definition: In order to
facilitate corecursive definitions of tries that are certified by
Agda's productivity checker, we equip the type of tries $\Lang\,i$
with an index $i : \Size$.  Index $i$ denotes an ordinal $\leq \omega$
corresponding to the \emph{definedness depth} of a trie
$t : \Lang\,i$.  Ultimately, we are interested only in fully defined
tries $t : \Lang\,\infty$, where $\infty$ is syntax for ordinal
$\omega$.  This means we can query $t$'s nodes at arbitrary
depth.  For a finite definedness level $i$ we can only inspect
nodes up to depth $i$.  In particular $t : \Lang\,0$ allows us to look
only at the root label $\tnu\,t$, its subtrees via $\tdelta\,t\,a$ are
undefined and Agda's type checker will object to such an expression.

If a trie $t : \Lang\,i$ can be defined by reference to tries of type
$\Lang\,j$ for $j<i$, written $j : \SizeLt i$, then
$t : \forall\{i\} \to \Lang\,i$ by well-founded recursion on ordinals,
which is our incarnation of a corecursive definition.
%
\aLang{}
%
As typing of the projections from tries we get
\[
\begin{array}{lll}
\tnu & : \forall\{i : \Size \} \to \Lang\,i \to \Bool \\
\tdelta & : \forall\{i : \Size\} \to \Lang\,i \to \forall\{j : \SizeLt i\} \to
          A \to \Lang\,j \\

\end{array}
\]
with hidden size arguments which will, if type checking succeeds, be
figured out by Agda's unifier and size constraint solver.
When taking the derivative $\tdelta\,\{i\}\,t\,\{j\}\,a$ we are free
to choose any $j$ strictly below $i$.  This expresses that if $t$ was
only defined up to depth $i$, then its $a$-subtree is less
defined; and we are allowed to waste information and chose a smaller
$j$ than necessary.  Wasting is fine since $\Lang\,i$ is a subtype of
$\Lang\,j$ whenever $i \geq j$, as we can always us a \emph{more} defined
value $t : \Lang\,i$ when a value of $\Lang\,j$ is demanded.  The
antitone subtyping chain
\[
  \Lang\,\infty \leq \dots \leq \Lang\,(\uparrow i) \leq \Lang\,i \leq
  \dots \Lang\,0
\]
can be justified by the equation
$\Lang\,i \cong \Bool \times \bigcap_{j<i} (A \to \Lang\,j)$
which holds semantically.

The isomorphism $\Lang\,i \cong \List\,i\,A \to \Bool$ is witnessed by
the following two functions. The first, $l \ni \vas$, checks membership
of word $\vas$ in language $l$ represented as a trie.  The empty word
$[\,]$ is in the language if $\tnu\,l$, \ie, if the language is
nullable.  The composite
word $a :: \vas$ is accepted by $l$ if $\vas$ is accepted by the
derivative $\tdelta\,l\,a$.
\ani{}%
Visually spoken, $l \ni \vas$ returns the root label of the subtree
selected by path $\vas$.

The second function constructs a trie representation $\tlang\,f$ from
the functional representation $f$ of a decidable language.  The trie
is constructed corecursively by copattern matching
\citep{abelPientkaThibodeauSetzer:popl13}.
\alang{}%
The root label $\tnu\,(\tlang\,f)$
is determined by whether $f$ accepts the empty word $[\,]$.
The $a$-derivative $\tdelta\,(\tlang\,f)\,a$ is constructed by
corecursion on the $a$-derivative of $f$.  The justification of the
recursive call to $\tlang$ is apparent once we make the hidden size
arguments visible:
\[
  \tdelta\;\{i\}\;(\tlang\,\{i\}\,f)\;\{j\}\;a = \tlang\;\{j\}\;\lambda \vas \to f\,(a :: \vas)
\]
By typing of the projection $\tdelta$, we have $j : \SizeLt i$, thus,
the definition of $\tlang\,\{i\}$ only rests on $\tlang\,\{j\}$ with a
smaller size index.  Well-founded induction on sizes guarantees that
the equation system has a unique solution.

The corecursive definition by copattern matching is sometimes likened
to differential equations \citep{hansenKupkeRutten}.  In the definition of
$\tlang$, the second equation ($\tdelta$) is the differential equation, and the
first equation ($\tnu$) determines the initial value.

\subsection{Constructing decidable languages by coiteration}
\label{sec:coit}

In the following, we implement some standard constructions on formal
languages by copattern matching.  These operations will allow us to
compute the trie of any regular expression.

The empty language $\tempty$ is the trie where each node label is
$\tfalse$.  Naturally, each subtree of $\tempty$ is again $\tempty$.
\aempty%
The language $\teps$ accepting only the empty word has root label
$\ttrue$ but all other labels are $\tfalse$.  Hence, any derivative is
the empty language.
\aeps%
The language $\tchar\,a$ accepting the one-letter word $a :: [\,]$ is
not nullable, its $a$-derivative is $\teps$ and all other derivatives
are $\tempty$.
\achar%
We obtain the language complement $\tcompl\,l$ of a language $l$ by
flipping all lables.  This is accomplished by recursing over the whole
tree.
\acompl%
Complement $\tcompl$ is a special instance of mapping a function
pointwise over all tree labels.

For the union $k \cup l$ of two languages $l$ and $k$, we overlay the
two tries and perform the Boolean disjunction operation on
corresponding node labels.
\aunion%
The intersection could be defined
analogously, using Boolean conjunction.  Both operations are instance
of a general $\tzipWith$-function that applies a binary operation
pointwise to a pair of tries.

All recursive definitions of tries so far have followed a specific pattern:
in the right hand sides of the recursive equations, the recursive call
was outermost, \ie, the equation had the form $\tdelta\,(g\,\vec y)\,x
= g\,\vec t$ for some variables $x,\vec y$ and some terms $\vec t$.
With the non-recursive equation being $\tnu\,(g\,\vec y) = o$,
this form is an instance of the commutative diagram for
terminal coalgebras and sometimes called \emph{coiteration}
\citep{geuvers:indtypes}.
This is the diagram for a $(\Bool \times (A \to \_))$-coalgebra
$(\Gamma, h)$ mapping into the terminal coalgebra $(\Lang, \langle \tnu, \tdelta \rangle)$:
\[
% \newcommand{\tnd}{\langle o,\; \lambda x \to \vec t \rangle}
\newcommand{\tnd}{\,h}
\xymatrix@R=12ex@C=12ex{
  \Gamma \ar[r]^(0.45)*{\tnd} \ar@{.>}[d]_*{%g\,\vec y \,:=\,
    \tcoit\tnd}
    & *!<-1.6ex,0ex>{~\Bool \times (A \to \Gamma)} \ar[d]^*{\tid \times (\tcoit\tnd\, \circ \_)}\\
  \Lang \ar[r]^(0.45)*{\langle \tnu, \tdelta \rangle}
    & *!<-3.2ex,0ex>{~\Bool \times (A \to \Lang)} \\
}
\]
With $g := \tcoit\,h$, the commutative law
% $\langle \tnu, \tdelta \rangle \circ \tcoit\,h = \tid \times (\tcoit\,h\,
% \circ \_) \circ h$.
\[\langle \tnu, \tdelta \rangle \circ g = \tid \times (g \circ \_) \circ h\]
can be applied to points $\vec y : \Gamma$ to yield
\[\langle \tnu, \tdelta \rangle (g\,\vec y)
  = (\tid \times (g \circ \_)) (h\,\vec y).\]
For our instance, $h\, \vec y= (o,\, \lambda x \to \vec t)$
with $\vec y \of \Gamma \der o : \Bool$ and $\vec y \of \Gamma, x \of A \der
t : \Lang$, thus,
\[\langle \tnu, \tdelta \rangle (g\,\vec y)
  = (o,\; g \circ (\lambda x \to \vec t)).\]
This can be split into the two equations
\[
\begin{array}{lll}
  \tnu\,(g\,\vec y) & = & o \\
  \tdelta\,(g\,\vec y)\,x & = & g\,\vec t \\
\end{array}
\]
that form the laws of a function
$g = \tcoit\;(\lambda \vec y \to (o,\, \lambda x \to \vec t))$
defined by coiteration (modulo some tupling and (un)currying).

The type/context $\Gamma$ can be interpreted as the set of states of
an automaton $h$ with a coupled presentation of the accepting state
set $\Gamma \to \Bool$ and the transition function $\Gamma \to (A \to \Gamma)$.
Function $\tcoit\,h$ maps a state $s : \Gamma$ to the language
$\tcoit\,h\,s$ accepted by $h$ starting from state $s$.
The discussed language constructions correspond to
constructions of (possibly infinite) automata
with references to existing automata as oracles. The
reader is invited to confirm this by expressing the given
constructions through coiteration.  Note however, that the state type $\Gamma$
might involve $\Lang$ and is, thus, not guaranteed to be finite!


\subsection{Constructing decidable languages by well-founded corecursion}

To complete the constructions of languages as supported by regular
expressions, we are missing language concatenation and the Kleene
star.  These can be constructed by \emph{corecursion up-to} which can
be reduced to primitive corecursion into a trie with an extended
alphabet \citep{traytel:fscd16}.  However, using sized types we can
naturally define these operations by their derivative laws, using
well-founded recursion on sizes.

Language concatenation $k \cdot l$ is our first non-trivial operation on languages.
The intuition
$(k \ni \vas) \wedge (l \ni \vbs) \imp (k \cdot l) \ni (\vas \oapp \vbs)$
leads to a specification
$(k \cdot l) \ni \vcs
  = \exists n.\; k \ni (\ttake\,n\,\vcs) \wedge l \ni (\tdrop\,n\,\vcs)
$
that does not suggest a pretty implementation \citep{doczkalKaiserSmolka:cpp13}.

We can try to understand language concatenation as an operation on the
tries $k$ and $l$.  If we think about accepting a word $\vcs$ in $k \cdot l$
by following pathes in $k$ and $l$, the following procedure applies:
We start by following branches in $k$.  Whenever we reach an accepting
node in $k$ we may decide that we have reached the boundary between
the words $\vas$ in $k$ and $\vbs$ in $l$ that make up the word $\vcs
= (\vas \oapp \vbs)$ in $k \cdot l$ and start following branches in
$l$.  However, since we are not sure we already reached the boundary,
we simultaneously continue to follow branches in $k$.  At each
accepting node in $k$ we spawn off a run in $l$.  Thus, a trie for $k
\cdot l$ may be constructed by the following operation on all
accepting nodes of $k$:  make the node non-accepting but then union the
subtree starting here with $l$.  This transformation is achieved by
the following corecursive definition of concatenation:
\aconcat
The concatenation of two languages is nullable iff both are nullable.
For the $x$-derivative, we follow the $x$-branch in $k$
via $\tdelta\,k\,x \cdot l$ in any case.  If the node is accepting $\tnu\,k$,
we may in addition follow the $x$-branch in $l$ via $\tdelta\,l\,x$.
As before, the equations for language concatenation correspond to the
derivation laws of regular expressions, but we arrived there by the
trie intuition.

The above definition is not an instance of coiteration for two
reasons:  First, the outermost call is to $\tifthenelse$ rather than
the recursive call $\kpl$.  Even if we consider $\tifthenelse$ to be special
(rather than just an arbitary Agda function), there is still a
recursive call $\kpl$ in the then-branch which is not at top-level,
but under the union-operator.  This
problem is usually fixed by defining a scheme for corecursion up to
union.  However, looking at the involved sizes we can accept the
definition in the present form as an instance of well-founded corecursion.
Crucial here is the sized typing of the union
\[
  \_\tcup\_ : \forall\{i\} (k\, l : \Lang\,i) \to \Lang\,i
\]
which asserts that the argument are no deeper analyzed than the
definedness depth of the result.  If we make all hidden size arguments
visible---having to switch to prefix operators instead of infix ones---%
we can see the propagation of definedness depth levels to the
recursive call $\kpl$.
\aconcatexpl%
Since the recursive call happens at smaller index $j < i$, it is justified.
Note also that in the definition of $\kpl$, last letter, $l : \Lang\,i$ is
casted to $\Lang\,j$ which is valid since $j < i$.

The iteration $l^*$ of a language $l$, aka \emph{Kleene star}, can be
informally described as ``zero or more repetitions of $l$''.  If for
some $n \geq 0$ we have words $\vas_1, \vas_2, \dots \vas_n \in l$, then
$(\vas_1 \oapp \vas_2 \oapp \dots \oapp \vas_n) \in l$.  In terms of tries, $l^*$
is obtained from $l$ by making the root accepting and unioning $l$
with any
subtree of $l$ that has an accepting root.  Intuitively, this means that at each
accepting node we may ``jump back'' to the root.  The corecursive definition
\astar%
relies on the sized typing of concatenation to justify the recursive call.

\section{Proving the Kleene Algebra Laws}
\label{sec:kleene}

In this section, we prove that decidable languages as introduced in
Section~\ref{sec:lang} form a \emph{Kleene algebra}.

\subsection{A Family of Equivalence Relations over Languages}
\label{sec:fameq}

Equality of tries, sometimes called \emph{strong bisimilarity} is
defined coinductively as follows.  Two tries are strongly bisimilar if
they have the same root and corresponding subtries are strongly
bisimilar in turn.  In Agda, this amounts to the following coinductive
definition:
\abisim
For one, note that we are relating tries $l,k : \Lang\,\tinfty$ whose
definedness depth is unbounded ($\tinfty$).  This means that any
subtrie such as $\tdelta\,l\,a$ is defined and in turn has type
$\Lang\,\infty$.

However, the relation itself is indexed by a definedness depth $i$.
In fact we are defining a family of types such that $\bisim l j k$
is a subtype of $\bisim l i k$ whenever $i \leq j$.  The depth is a
lower bound on how far the proof of equality of $l$ and $k$ is
constructed.  In particular, we can only inspect the derivation
$\cdelta\,p\,a$ of a proof $p : \bisim l i k$ if $i > 0$.  As for
coinductive types like $\Lang\,i$, the size index $i$ is just a tool
for the corecursive construction of derivations.  Ultimately, we are
only interested in fully defined equality proofs $p : \bisim l \tinfty k$.
In particular, this is not to be confused with \emph{ordered families
  of equivalences} (OFEs) \citep{gianantonioMiculan:types02} $l \equiv_n k$
which refine the notion of equality itself.  (There, $l \equiv_0 k$
would hold always and $l \equiv_{n+1} k$ would hold if $l$ and $k$
have equal roots and their immediate subtries are $\equiv_n$-related.)
OFEs are a different approach to justifying corecursive definitions.

Each of the coinductive relations forms an equivalence relation,
proven for the whole family by coiteration.  For reflexivity, we have
to prove that given a trie $l$, we can construct a derivation that $l$
is strongly bisimilar to itself $l$, up to arbitrary depth $i$.
\arefl
The proof $\tcongrefl$ of $\bisim l i l$ is constructed lazily.  If we
are asking for its first component $\congnu\,\tcongrefl$ we get a proof
that the root $\tnu\,l$ is identical to itself, namely $\trefl :
\tnu\,l \,\tequiv\, \tnu\,l$.  If we are asking for the $a$-branch of
its second component, $\congdelta\,\tcongrefl\,a$ at depth $j < i$, it
computes $\tcongrefl : \bisim l j l$ corecursively.

Symmetry is similarly defined.  To compute a proof of $k \cong l$
up to depth $i$, we only need a derivation of $l \cong k$ up to depth
$i$; thus, the type of $\tcongsym$ is $\bisim l i k \to \bisim k i l$.
\asym
Transitivity is likewise depth preserving.  Depth-preservation is
crucial to combine reasoning by transitivity and the coinductive
hypothesis in a natural way, as we will see below.
\atrans
Taken together, each $\bisim \_ i \_$ is an equivalence relation, and
forms a setoid $\Bis\,i$ with carrier $\Lang\,\tinfty$.
\asetoid
Later, we will use these setoids to reason by equality chains.

\subsection{Laws of language union}
\label{sec:lawunion}

Decidable languages form an idempotent commutative monoid under
union.  The individual laws, like associativity, commutativity,
idempotency, and unit, follow from the corresponding laws of the
boolean disjunction, which are pointwise applied at all the
corresponding nodes of the involved tries.  In Agda, these are direct
proofs by coiteration.
\aunionassoc
\aunioncomm
\aunionidem
\aunionemptyl
% \aunionemptyr
Finally, union preserves equality, which is again proven by
coiteration.
The sized typing will be crucial to apply a coinductive hypothesis
under $\tunioncong$ later.
\aunioncong
A derived law we require later is that union distributes over itself.
Now that we have established that union fullfills the laws of an
idempotent commutative monoid, we can use a solver to prove this law
rather automatically by reflection.
\aunionuniondistr
Concretely, the solver checks that both sides of the equation have the
same set of atoms, by normalizing both sides to the set $\{k,l,m\}$.
This solver is implemented in Agda itself, but we will not describe it
further here.\footnote{%Implementation see
\url{https://github.com/agda/agda-stdlib/blob/1c78e4e/src/Algebra/IdempotentCommutativeMonoidSolver.agda}
implements this solver.
}

\subsection{Laws of language concatenation}
\label{sec:lawcat}

Concatenation distributes over union, for instance,
$k \cdot (l \tunion m) \cong (k \cdot l) \tunion (k \cdot m)$.
Naturally, we would like to prove this by coinduction.  The case for
$\tnu$ follows by the boolean distributivity law
$x \wedge (y \vee z) = (x \wedge y) \vee (x \wedge z)$.  In the case
for $\tdelta$, we would like to reason by the following equality
chain.  We consider the subcase that $k$ is nullable.
\[
\begin{array}{cl@{\qquad}r}
\derive{(k \cdot (l \tunion m))} a
  & \cong & \mbox{by definition} \\
\derive k a \cdot (l \tunion m) \tunion \underline{\derive{(l \tunion m)} a}
  & \cong & \mbox{by definition} \\
\underline{\derive k a \cdot (l \tunion m)} \tunion (\derive l a \tunion \derive m a)
  & \cong & \mbox{by coinduction hypothesis} \\
((\derive k a \cdot l) \tunion \underline{(\derive k a \cdot m)}) \tunion (\underline{\derive l a} \tunion \derive m a)
  & \cong & \mbox{by union laws} \\
\underline{((\derive k a \cdot l) \tunion \derive l a)} \tunion \underline{((\derive k a \cdot m) \tunion \derive m a)}
  & \cong & \mbox{by definition} \\
\derive{(k \cdot l)} a \tunion \derive{(k \cdot m)} a
  & \cong & \mbox{by definition} \\
\derive{(k \cdot l) \tunion (k \cdot m)} a
\end{array}
\]

% \begin{figure}[htbp]
%   \centering
% %\hrule width0.8\textwidth
% \aconcatuniondistribl
%   \caption{Concatenation distributes over union.}
%   \label{fig:concatuniondistribl}
% \end{figure}

\begin{figure}[htbp]
  \centering
%\hrule width0.8\textwidth
\aconcatuniondistribr
  \caption{Concatenation distributes over union.}
  \label{fig:concatuniondistribr}
\end{figure}

\section{Constructing Automata}
\label{sec:aut}

\section{Conclusions}
\label{sec:concl}

\bibliographystyle{elsarticle-harv}
\bibliography{auto-jlamp17}

\end{document}

\endinput
%%
%% End of file
