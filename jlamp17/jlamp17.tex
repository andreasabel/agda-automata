%\errorstopmode
%\nonstopmode
\documentclass[preprint,12pt,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{ccicons}
\usepackage{xspace}

\usepackage{bbm}
\usepackage[usenames,dvipsnames]{color}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=Red, citecolor=ForestGreen,
  urlcolor=MidnightBlue}
\usepackage{natbib}
\usepackage{cleveref}

%\usepackage[postscript,mathletters]{ucs}
\usepackage{src/latex/agda}
% Andreas: auto-generated not needed now
% \usepackage[postscript,autogenerated]{ucs}
\usepackage{pifont}
\usepackage{textgreek}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}
%\usepackage{newunicodechar} % needs utf8 not utf8x
\usepackage{autofe}

%\usepackage{myagda}
\AgdaNoSpaceAroundCode{}
%\renewcommand{\AgdaSymbol}[1]{\AgdaNoSpaceMath{\AgdaFontStyle{\textcolor{AgdaSymbol}{\ensuremath{#1}}}}}

%\renewcommand{\AgdaIndentSpace}{\qquad\qquad} % NO EFFECT

% \newunicodechar{ˡ}{\ensuremath{{}^{\mathsf{l}}}}
% \newunicodechar{ˢ}{\ensuremath{{}^{\mathsf{s}}}}
% \newunicodechar{δ}{\ensuremath{\delta}}
% \newunicodechar{ε}{\ensuremath{\varepsilon}}
% \newunicodechar{λ}{\ensuremath{\lambda}}
% \newunicodechar{ν}{\ensuremath{\nu}}
% \newunicodechar{ᶜ}{\ensuremath{{}^{\mathsf{c}}}}
% \newunicodechar{ᵢ}{\ensuremath{{}_i}}
% \newunicodechar{′}{\kern0.07em\ensuremath{'}}
% \newunicodechar{ₒ}{\ensuremath{{}_o}}
% \newunicodechar{ₙ}{\ensuremath{{}_{n}}} % here we use italics
% \newunicodechar{∀}{\ensuremath{\forall}}
% \newunicodechar{∅}{\ensuremath{\emptyset}}
% \newunicodechar{∋}{\ensuremath{\ni}}
% \newunicodechar{∎}{\ensuremath{\scriptstyle{\blacksquare}}}
% \newunicodechar{∧}{\ensuremath{\wedge}}
% \newunicodechar{∨}{\ensuremath{\vee}}
% \newunicodechar{∪}{\ensuremath{\cup}}
% \newunicodechar{≟}{\ensuremath{\stackrel{?}{=}}}
% \newunicodechar{∷}{\ensuremath{::}}
% \DeclareUnicodeCharacter{8759}{\ensuremath{::}}
% \newunicodechar{▹}{\ensuremath{\vartriangleright}}

%\newcommand{\DeclareUnicodeCharacter}{\newunicodechar}
\DeclareUnicodeCharacter{02B3}{\ensuremath{{}^{\mathsf{r}}}}
\DeclareUnicodeCharacter{02E1}{\ensuremath{{}^{\mathsf{l}}}}
\DeclareUnicodeCharacter{02E2}{\ensuremath{{}^{\mathsf{s}}}}
\DeclareUnicodeCharacter{03B4}{\ensuremath{\delta}}
\DeclareUnicodeCharacter{03B5}{\ensuremath{\varepsilon}}
\DeclareUnicodeCharacter{03BB}{\ensuremath{\lambda}}
\DeclareUnicodeCharacter{03BD}{\ensuremath{\nu}}
\DeclareUnicodeCharacter{1D9C}{\ensuremath{{}^{\mathsf{c}}}}
%\DeclareUnicodeCharacter{1D52}{\ensuremath{{}^{\mathrm{o}}}} % UNUSED
\DeclareUnicodeCharacter{1D62}{\ensuremath{{}_i}}
\DeclareUnicodeCharacter{2032}{\kern0.07em\ensuremath{'}}
\DeclareUnicodeCharacter{207A}{\ensuremath{{}^+}}
\DeclareUnicodeCharacter{2080}{\ensuremath{{}_0}}
\DeclareUnicodeCharacter{2081}{\ensuremath{{}_1}}
\DeclareUnicodeCharacter{2082}{\ensuremath{{}_2}}
\DeclareUnicodeCharacter{2092}{\ensuremath{{}_o}}
% ALT: rm variant (Andreas: I think italics looks better)
%\DeclareUnicodeCharacter{1D62}{\ensuremath{{}_{\mathrm{i}}}}
%\DeclareUnicodeCharacter{2092}{\ensuremath{{}_{\mathrm{o}}}}  % or italics?
\DeclareUnicodeCharacter{2099}{\ensuremath{{}_{n}}} % here we use italics
\DeclareUnicodeCharacter{2200}{\ensuremath{\forall}}
\DeclareUnicodeCharacter{2205}{\ensuremath{\emptyset}}
\DeclareUnicodeCharacter{220B}{\ensuremath{\ni}}
\DeclareUnicodeCharacter{220E}{\ensuremath{\scriptstyle{\blacksquare}}}
\DeclareUnicodeCharacter{221E}{\ensuremath{\infty}}
\DeclareUnicodeCharacter{2227}{\ensuremath{\wedge}}
\DeclareUnicodeCharacter{2228}{\ensuremath{\vee}}
\DeclareUnicodeCharacter{222A}{\ensuremath{\cup}}
\DeclareUnicodeCharacter{2237}{\ensuremath{::}}
\DeclareUnicodeCharacter{225F}{\ensuremath{\stackrel{?}{=}}}
\DeclareUnicodeCharacter{2245}{\ensuremath{\cong}}
\DeclareUnicodeCharacter{2248}{\ensuremath{\approx}}
\DeclareUnicodeCharacter{2261}{\ensuremath{\equiv}}
\DeclareUnicodeCharacter{2295}{\ensuremath{\mathord{\oplus}}}
\DeclareUnicodeCharacter{22A4}{\ensuremath{\mathord{\top}}}
\DeclareUnicodeCharacter{22A5}{\ensuremath{\mathord{\perp}}}
\DeclareUnicodeCharacter{230A}{\ensuremath{\lfloor}}
\DeclareUnicodeCharacter{230B}{\ensuremath{\rfloor}}
\DeclareUnicodeCharacter{25B9}{\ensuremath{\vartriangleright}}

\usepackage[all]{xy}
\input{macros}

% \makeatletter
% \newcommand{\setword}[2]{%
%   \phantomsection
%   #1\def\@currentlabel{\unexpanded{#1}}\label{#2}%
% }
% \makeatother

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{JLAMP}
%\journal{Journal of Logic and Algebraic Methods in Programming}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{Equational Reasoning about Formal Languages in Coalgebraic Style
}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}

\author{Andreas Abel}

\address{Department of Computer Science and Engineering, Gothenburg University}


\begin{abstract}

Formal languages and automata are a foundational topic of computer
science, with many practical applications such as compiler
construction, textual search, model checking, and decidability of
certain logics.  Automata are an instance of transition systems which
have the structure of a coalgebra, and coalgebraic and coinductive
reasoning tools such as simulations, bisimulations, and up-to
techniques have been successfully employed to study formal languages.

In this paper, we show how routine reasoning about formal languages
can be carried out with just the coinductive notion of equality of
languages aka bisimilarity.  We formalize a coinductive type of
languages and the coinductive type family of strong bisimilarity of
languages in the proof assistant Agda using sized types.  The sized
typing enables us to establish algebraic properties of language
operations through coinductive proofs of bisimilarity by equational
reasoning.  In particular, after verifying that formal languages form
a Kleene algebra, the laws of the Kleene algebra are sufficient to
prove correctness of the usual constructions on automata.

\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Automaton \sep coalgebra \sep coinduction \sep copattern
\sep formal~language \sep sized types \sep type theory.
%Automaton; coalgebra; coinduction; copattern; formal~language; sized types; type theory.

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
\MSC[2010]
03B15 \sep % Higher-order logic and type theory
68N18 \sep % Functional programming and lambda calculus
68Q45 \sep % Formal languages and automata [See also 03D05, 68Q70, 94A45]
68Q70      % Algebraic theory of languages and automata
\end{keyword}

\end{frontmatter}

%\tnotetext{}
\ccbyncnd{} This paper is published under a CC-BY-NC-ND license.
\begin{center}
It is recommended to print this article in
\definecolor{r1}{RGB}{0,129,188}%
\definecolor{r2}{RGB}{252,177,49}%
\definecolor{r3}{RGB}{35,34,35}%
\definecolor{r4}{RGB}{0,157,87}%
\definecolor{r5}{RGB}{238,50,78}%
{\color{r1}c}%
{\color{r2}o}%
{\color{r3}l}%
{\color{r4}o}%
{\color{r5}r}%
% {\color{Magenta}c}%
% {\color{Brown}o}%
% {\color{ForestGreen}l}%
% {\color{BurntOrange}o}%
% {\color{MidnightBlue}r}%
.
\end{center}


%% \linenumbers

\section{Introduction}
\label{sec:intro}

Formal languages and automata are a foundational topic of computer science, with
many practical applications such as compiler construction, textual search, model
checking, and decidability of certain logics.
While the theory of formal languages and automata goes back to the
1960s, its connection to the coalgebras of category theory is more
recent.  This coalgebraic formulation suggests that \emph{coinduction} should be
central for reasoning about formal languages and
automata.  In particular, to show that two automata recognize the same
language, one can find a bisimulation between these automata and then
use coinduction in the form of
the theorem of Knaster and Tarski \citeyearpar{tarski:fixpoint}
for the greatest fixed point.  However, coming up with a suitable
bisimulation is
so far an act of creativity and has not been automated, although
remedies like parameterized coinduction
\citep{hurNeisDreyerVafeiadis:popl13}
have been suggested for a smoother experience of formal coinductive
reasoning.   Further, a set of standard strategies to find a
bisimulation have become known as \emph{coinduction-up-to techniques}
\citep{pousSangiorgi:book12}.

In this article, we demonstrate that a generalization of primitive
coinduction to \emph{well-founded coinduction} as both a definition
principle and a proof principle lets us carry through a substantial
part of basic automata theory with just \emph{equational reasoning}
over coinductive equality (bisimilarity).
Technically, we rely on a formulation of coinduction with sized types
\citep{hughesParetoSabry:popl96,amadio:guardcondition,gimenez:typebased,abel:lmcs07,sacchini:lics13,abelPientka:jfp16}
in the context of Martin-Löf Type Theory~\citeyearpar{martinlof:predicative75}.
This enables us to define the Kleene algebra operations of formal
languages elegantly
via their Brzozowski derivatives \citeyearpar{brzozowski:jacm64}.
Further, we can define coinductive language equality in a way that
gives us the ability to prove theorems by coinduction and equation
chains.
In particular, it allows us to apply the coinductive hypothesis under
the proof term for transitivity.  The latter is not possible in weaker
formulations of coinduction such as the \emph{Calculus of
  (Co)inductive Constructions} underlying the Coq \citeyearpar{inria:coq88}
proof assistant, which makes use of an untyped guardedness check
\citep{coquand:infiniteobjects}.
As an alternative to sized types, well-founded coinduction could
probably be based on ordered families of equivalences
\citep{matthews:tphols99,gianantonioMiculan:types02}
or ultrametric spaces.
A type theory utilizing these approaches is emerging
\citep{bizjakGrathwohlCloustonMogelbergBirkedal:fossacs16},
but has not seen a mature implementation yet.

Agda \citeyearpar{agdawiki} is currently the only type-theoretic proof
assistant with support for sized types.  Albeit still experimental,
Agda's sized types let us formalize decidable languages and automata
elegantly through definition by copattern matching
\citep{abelPientkaThibodeauSetzer:popl13}.  All the definitions,
theorems, and proofs of this paper have been extracted from Agda code
via an Agda to LaTeX translation and are, thus, guaranteed to be
correct (assuming the consistency of Agda itself).

This article does not present any new or surprising results about
formal languages or automata but is solely concerned about the
techniques for formal definition and proof.  It should, thus, be seen
as a tutorial.  It has been inspired by Rutten's tutorial on
coalgebraic techniques for formal languages \citeyearpar{rutten:concur98}
and Traytel's recent formalizations in Isabelle
\citeyearpar{traytel:fscd16}.  Traytel reports that Isabelle is being
extended by \emph{friendly} coinductive definitions that bring part of
the reasoning power of sized types, possibly sufficient to mimic the
reasoning style employed in this article.

\para{Overview}  In Section~\ref{sec:prelim} we briefly
recapitulate the bits and pieces of Type Theory and Agda relevant for
this article.  In Section~\ref{sec:lang} we define decidable languages
as infinite tries and some operations on them in the style of
Brzozowski derivatives.  Subsequently, in Section~\ref{sec:kleene} we
prove the Kleene algebra laws for these operations.  The final technical
section, \ref{sec:aut}, is devoted to the corresponding constructions
on automata and their correctness.

% standard proof techniques,
% such as:
% \begin{enumerate}
% \item Equational reasoning
% \end{enumerate}

% Automata are an instance of transitions systems which have the
% structure of a coalgebra.
% Coalgebraic and coinductive reasoning tools such as simulations and bisimulations have
% been successfully employed to study formal languages. % \cite{}.
% Coinduction-up-to techniques have emerged as the state of the art to
% get coinductive proofs through. % \cite{sangiorgi}.

% This paper presents an anti-thesis to up-to techniques.  We show how
% much of the reasoning about formal languages can be carried out already with
% the coinductive notion of equality of languages aka bisimilarity.  We
% formalize a coinductive type of languages and the coinductive type
% family of strong bisimilarity of languages in Agda using sized types.
% The sized typing enables us to do coinductive proofs of bisimilarity
% by equational reasoning as customary to establish algebraic
% properties.  In particular, after establishing that formal languages
% form a Kleene algebra, the laws of the Kleene algebra are sufficient
% to prove correctness of the usual automata constructions.


\section{Preliminaries: Type Theory and Agda}
\label{sec:prelim}

\input{latex/Agda}

The definitions and theorems of this article are formulated in
Dependent Type Theory \`a la \cite{martinlof:predicative75}, in
particular, in the Agda \citeyearpar{agdawiki} language.  Agda is an
implementation of Type Theory with several extensions; we will most
notably make use of \emph{sized types}
\citep{hughesParetoSabry:popl96,abelPientka:jfp16} to express
coinductive types and definitions by coinduction.

On the one hand, Agda is a dependently-typed purely functional programming
language, and on the other hand, thanks to the Curry-Howard
correspondence, a proof assistant for constructive logic.  In the
following, we will briefly introduce the syntax of Agda by example.
The experienced Agda user can safely skip the remainder of this section.

% - Curry-Howard isomorphism
% - function computable

\subsection{Agda as a dependently-typed functional language}
\label{sec:agda}

The core features of Agda are inductive families
\citep{dybjer:fac94} and functions defined by pattern
matching.  A very simple inductive family is the enumeration type
$\Bool$ with two constructors $\ttrue$ and $\tfalse$.

\begin{minipage}{\linewidth}
\abool
\end{minipage}

\noindent
$\Bool$ itself has type $\Set$, which is a universe or a type of types, but not all
types, to avoid vicious cycles.  For instance $\Set$ itself does not
have type $\Set$, but inhabits the next universe $\agda{Set\ensuremath{_1}}$.
Boolean negation can be defined by simple pattern matching.
\anot

%\begin{minipage}{0.5\linewidth}
%\end{minipage}
%\begin{minipage}{0.5\linewidth}
%\end{minipage}

\noindent
Agda also supports Unicode and infix and mixfix operators.
For example, we can define Boolean disjunction like this:

\avee

The notation $(a\;b : \Bool) \to \Bool$ is short
for $(a : \Bool) (b : \Bool) \to \Bool$
or $(a : \Bool) \to (b : \Bool) \to \Bool$,
which is the syntax for dependent function types.
In this case, there is no actual dependency,
since the bound variables $a$ and $b$ are not
subsequently used in the type.

A function can be made polymorphic by taking a type $A : \Set$ as
input and having argument and result types depend on $A$.
We typically want to omit the type argument, so we place it in braces,
as in the following definition:

\aapplyWhen

Data types can be parameterized over other types $A$.  For instance
$\Maybe\,A$ embeds an arbitrary type $A$ via $\tjust$ and extends it
by a new value $\tnothing$.

\amaybe

Data types can be recursive, such as $\List\,i\,A$ parameterized by a
size $i$ and a type $A$.  The size $i : \Size$ acts as an upper
bound on the length of the list.  The special size $\tinfty : \Size$
means unbounded length.

\begin{minipage}{\linewidth}
\alist
\end{minipage}

\noindent
A list can be empty (constructor $\tnil$); then any size $i$ is an
upper bound on its length.  Or, the list is nonempty (\eg,
$x \tcons \vxs$);  in this case, if $j$ is an upper bound on the length of
its tail $\vxs$ and $j < i$, then $i$ is an upper bound on the length
of $x \tcons \vxs$.  Strictly speaking, the constructor $\tconsu$
takes three arguments: $j : \SizeLt i$ and $x : A$ and $\vxs :
\List\,A\,j$, but the first argument $j$ is in $\{$braces$\}$ and thus
declared as hidden.  The user does not have to write it, and its value
will be inferred by Agda if possible.  Note that $j$ occurs in the
type of $\vxs$, thus, this is a proper dependency.

It is possible to supply a hidden argument to a function by enclosing it in
braces.  In case of the infix operator $\tconsu$, we have to fall back
to prefix style $\tconsu\,\{j\}\,x\,\vxs$ though.

The types $\Size$ and $\SizeLt\,i$ are Agda primitives that are used
for termination checking.  For instance, consider the mapping function
on lists.  Note that the notation $\forall\{i\, A\, B\} \to \dots$ is
short for $\{i : \_\} \{A : \_\} \{B : \_\} \to \dots$ and becomes
$\{i : \Size\} \{A : \Set\} \{B : \Set\} \to \dots$ after type
reconstruction.

\amap

\noindent
Function $\tmap\,f$ takes a list of size $i$ as input and returns a list with the same
upper bound.  We say $\tmap$ is \emph{size preserving}.  As $\tmap$ is
defined recursively, termination is not obvious.  Agda infers from
pattern $x \tcons \vxs : \List\,i\,A$, which is short for pattern
$\tconsu\,\{j\}\,x\,\vxs : \List\,i\,A$ with a pattern variable $j$
introduced by Agda, that $\vxs : \List\,j\,A$ and $j : \SizeLt i$.
Consequently, the recursive call $\tmap\,f\,\vxs$---which internally
expands to $\tmap\,\{j\}\,f\,\vxs$--- is justified, by the descent in
size $j < i$.  An analogous argument assures termination of $\tfoldr$,
the iteration principle for lists, which replaces in a list the $\tnil$
constructor by $n : B$ and any $\tconsu$ constructor by $c : A \to B
\to B$.

\afoldr

\noindent
As an application of $\tfoldr$ and $\tmap$, we define $\tany\,p\,\vxs$
which is $\ttrue$ if $p\,x$ is $\ttrue$ for any element $x$ of $\vxs$.

\aany

Finally, data types with a single constructor can alternatively be defined as
record types.  For instance, the Cartesian product $A \ttimes B$ can be
implemented as record type with the projections
% $\tfst : \forall\{A\,\B\} \to A \ttimes B \to A
$\tfst : A \ttimes B \to A$ and
$\tsnd : A \ttimes B \to B$ and
constructor $\AgdaInductiveConstructor{\_,\_} : A \to B \to A \ttimes B$.

\aprod

Agda allows the projections also on the left hand sides of
definitions by pattern matching.  In the following, we define for a
pair $p : A \ttimes B$ its reversal $\tswap\,p : B \ttimes A$ by giving
its value for all valid projections.  This definition form is called
\emph{definition by copattern matching}, since we are not matching on
a function argument, but on the possible observations on the function
result \citep{abelPientkaThibodeauSetzer:popl13}.

\aswap

\noindent
This is, of course, just one possible implementation of $\tswap$,
which we chose to exemplify copattern matching.  A simple clause
$\tswap\,(a\,,\,b) = (b\,,\,a)$ would have done the job, but copattern
matching will be the definition principle of choice for coinductive
structures in Section~\ref{sec:lang}.

\subsection{Agda as a proof assistant}

Martin-L\"of Type Theory allows us to reason about programs via the
propositions-as-types paradigm.  A proposition is seen as the type of
its proofs, for instance, the absurd proposition $\tbot$
\emph{(Falsehood)} has no proof,
and the trivial proposition $\ttop$ \emph{(Truth)} has a proof with no
further content.

In Agda, $\tbot$ is modeled as a data type with no constructors.
Given a proof $p : \tbot$, we can prove any proposition $A$ (populate
any type $A$) by matching on $p$.  As there are no constructors of
$\tbot$, there is nothing further to show, indicated in Agda by the
absurd pattern $()$ which matches anything of empty type.

\abot

Truth $\ttop$ is modeled as record type with no fields.  There is no
information to extract from a proof of $\ttop$, a proof is simply an
empty record.

\atriv

Implication $A \to B$ coincides with the ordinary function space, and
universal quantification $\forall x \to A$ with the dependent
function space $(x : \_) \to A$.

We can define our own propositions, predicates, and relations as
inductive (or coinductive, see Section~\ref{sec:fameq}) families.  The
prime example is \emph{propositional equality} $x \tequiv y$ of
objects $x,y : A$, which is defined as a data type with a hidden
parameter $A : \Set$, a visible parameter $x : A$, and an index $y :
A$.  The only constructor $\trefl$---which has no arguments---fixes
$y$ to be identical to $x$, thus, witnesses that $x$ and $y$ are
identical modulo Agda's internal notion of equality (which is called
\emph{definitional equality}).

\aeq

\noindent
Since it is defined inductively, propositional equality is the smallest
relation on $A$ that is reflexive.

Proofs of equality can be used by
pattern matching.  For instance, we can prove symmetry of equality,
\ie, $x \tequiv y$ implies $ y \tequiv x$ by
pattern matching on the proof of $x \tequiv y$.

\aeqsym

\noindent
The only matching constructor $\trefl$ forces $y$ to be identical to
$x$.  This is indicated in Agda by the inaccessible pattern $.x$ which
means that $y$ has been instantiated by the term $x$.  As a
consequence, the goal becomes $x \tequiv x$ which is simply
proved by $\trefl$.

In a similar fashion, we prove transitivity of propositional
equality.  By matching both the proofs of $x \tequiv y$ and $y
\tequiv z$ against $\trefl$, variables $y$ and $z$ become
instantiated to $x$, and again, the goal becomes simply
$x \tequiv x$.

\aeqtrans

In general, inductively defined propositions are inhabited by proof
trees in the same way that inductive types are inhabited by trees.
% In Martin-L\"of Type Theory, these are the same thing.
As an example, consider the proposition $\Any\,i\,P\,\vxs$ which
states that predicate $P : A \to \Set$ holds on some element $x : A$
of (unbounded) list $\vxs : \List\,\tinfty\,A$.
Parameter $i : \Size$ is an upper bound
on the tree height of a proof $p : \Any\,i\,P\,\vxs$ of this
proposition.

\aAny

\noindent
Constructor $\tHere$ establishes the proposition for a non-empty list
$x \tcons \vxs$ given a proof $p : P x$ that predicate $P$ holds on
the head $x$ of the list.  The resulting proof tree is a leaf, and any
size $i$ is an upper bound on the height of this derivation.  Constructor
$\tThere$ takes a derivation $p : \Any\,j\,P\,\vxs$ stating that $P$ holds on
some element of list $\vxs$, and builds a proof of
$\Any\,i\,P\,(x \tcons \vxs)$.  The tree height of $p$, ordinal $j$,
is necessarily strictly smaller than $i$.

Proofs in Type Theory naturally contain the necessary information to
construct witnesses for existential propositions such as $\Any$.  In
this case, a proof takes the form $\tThere^n\,(\tHere\,p)$ where $n$
is the index of witnessing element $x$ that satisfies $P$, and $p : P\,x$ is
the evidence for the latter fact.



This concludes the short tutorial on Type Theory and Agda.  In the
next section, we introduce coinductive types for the example of
infinitely deep trees.  In the following, we will sometimes write
$\List\,A$ for unbounded lists $\List\,\infty\,A$.

\section{Decidable Languages, Coinductively}
\label{sec:lang}

Given an alphabet $A$, a word $\vas : \List\,A$ is a list of
characters.  A language over $A$ is usually described as set of words,
and a decidable language as such a set whose characteristic function
% $\List\,A \to \Bool$
is computable.  We will work in the setting of Type Theory
\citep{martinlof:predicative75}
where each function is computable, thus, we can identify a decidable
languages with its characteristic
function of type $\List\,A \to \Bool$ where $\Bool$ is
the two-element data type with constructors $\ttrue$ and $\tfalse$.

% A potentially infinite
A set of words with decidable membership can also be
represented as a \emph{trie}.  For our purposes,
a trie is an $A$-branching tree whose nodes are labeled by Booleans.
Any word $\vas$ is a path into the tree selecting a subtree.  The root label
of that subtree indicates the status of the word $\vas$.  Label $\ttrue$
means the word is member of the set, label $\tfalse$ means it is not a
member.  Even though each word is finite, the language might be
infinite, thus, tries have infinite depth in general.
In fact, the tries we use have \emph{only infinite} branches, regardless of
whether we represent a finite language or not.

For instance, let us consider the language $E$ of even natural numbers in
binary representation forbidding leading zeros.  Writing $0$ as $a$ and $1$ as $b$, our
language contains the words $a$, $ba$, $baa$, $bba$, $baaa$, $baba$, etc.
Given the alphabet $A = \{a,b\}$, the language can be concisely described by the
regular expression $a + b(a + b)^*a$.

\begin{figure}[htbp]
  \centering
\[
%  \entrymodifiers={++[o][F-]}
%  \SelectTips{cm}{}
  \xymatrix@C=6.6ex@R=0ex{
            &              &              &              & \cdots \\
            &              &              & *++[o][F-]{} \ar[ru]^a \ar[rd]^b&        \\
            &       & *++[o][F-]{} \ar[ru]^a \ar[rd]^b & & \cdots \\
            &              &              & *++[o][F-]{} \ar[ru]^a \ar[rd]^b&        \\
            & *++[o][F=]{} \ar[ruu]^a \ar[rdd]^b & & &     \cdots \\
            &              &              & *++[o][F-]{} \ar[ru]^a \ar[rd]^b&        \\
            &       & *++[o][F-]{} \ar[ru]^a \ar[rd]^b & & \cdots \\
            &              &              & *++[o][F-]{} \ar[ru]^a \ar[rd]^b&        \\
*++[o][F-]{} \ar[ruuuu]^a \ar[rdddd]^b & & & &             \cdots \\
            &              &              & *++[o][F=]{} \ar[ru]^a \ar[rd]^b&        \\
            &       & *++[o][F=]{} \ar[ru]^a \ar[rd]^b & & \cdots \\
            &              &              & *++[o][F-]{} \ar[ru]^a \ar[rd]^b&        \\
            & *++[o][F-]{} \ar[ruu]^a \ar[rdd]^b & & &     \cdots \\
            &              &              & *++[o][F=]{} \ar[ru]^a \ar[rd]^b&        \\
            &       & *++[o][F-]{} \ar[ru]^a \ar[rd]^b & & \cdots  \\
            &              &              & *++[o][F-]{} \ar[ru]^a \ar[rd]^b&        \\
            &              &              &              & \cdots \\
}
\]
  \caption{Trie of $E$}
  \label{fig:trie}
\end{figure}

Figure~\ref{fig:trie} shows an initial part of the trie of language
$E$, where double-circled nodes denote membership of the word leading
to that node, and single-circled ones non-membership.
Observe that the subtree $ba$ has only accepting nodes
along its left-most path $a^*$, witnessing that $ba^*$ is a
sublanguage of $E$ (representing $2^n$ for $n \geq 1$).
Thus, a finite trie would be insufficient to represent $E$.

Generalizing $\Bool$ to $B$,
the connection between
the type $T$ of $A$-branching $B$-labeled tries
and the type $\List A \to B$ can also be derived by calculating type
isomorphisms \citep{hinze:ggtries,altenkirch:tlca01}:
\[
\begin{array}{lcl}
  \List A \to B
     & \cong & (1 + A \times \List A) \to B
\\   & \cong & (1 \to B) \times ((A \times \List A) \to B)
\\   & \cong & B \times (A \to (\List A \to B))
\end{array}
% \begin{array}{lcl}
%   \List A \to B
%      & =     & (\mu X.\,1 + A \times X) \to B
% \\   & \cong & \nu Y. (1 + A \times Y) \to B
% \\   & \cong & \nu Y. (1 \to B) \times ((A \times Y) \to B)
% \\   & \cong & \nu Y. B \times (A \to Y) \to B)
% \end{array}
\]
This means that $\List A \to B$ is a
solution of the recursive equation
\[
  X \cong B \times (A \to X)
\]
which also describes the decomposition of a trie $X$ into its root label of
type $B$ and its $A$-indexed family of subtrees $A \to X$.
Tries $T$ are the greatest solution of this equation and we write
$T = \nu X.\, B \times (A \to X)$.  We will later establish the
isomorphism between $T$ and $\List A \to B$ more precisely.

% It is
% isomorphic to the $B$-valued functions over $\List A = \mu X.\,1 + A
% \times X$, as can be seen
% by a simple calculation:
% \[
% \begin{array}{lcl}
%   \List A \to B
%      & \cong & (1 + A \times \List A) \to B
% \\   & \cong & (1 \to B) \times ((A \times \List A) \to B)
% \\   & \cong & B \times (A \to (\List A \to B))
% \end{array}
% % \begin{array}{lcl}
% %   \List A \to B
% %      & =     & (\mu X.\,1 + A \times X) \to B
% % \\   & \cong & \nu Y. (1 + A \times Y) \to B
% % \\   & \cong & \nu Y. (1 \to B) \times ((A \times Y) \to B)
% % \\   & \cong & \nu Y. B \times (A \to Y) \to B)
% % \end{array}
% \]
% Since $\List A \to B$ solve

\subsection{Coinductive tries in Agda}
\label{sec:trieAgda}

\input{latex/Language}

In Agda, we represent the coinductive type
$\nu X.\, \Bool \times (A \to X)$ of tries
as a coinductive record type $\Lang$
with fields $\tnu : \Bool$ for the root label
and $\tdelta : A \to \Lang$ for the family of subtrees.
If field $\tnu$ is $\ttrue$ then the language contains the empty word
and is sometimes called a \emph{nullable} language, hence the field
name $\tnu$.

The name $\tdelta$ is inspired by its role as
\emph{Brzozowski derivative}.
Given a decidable language $f : \List A \to \Bool$, its $a$-derivative
$\delta\,f\,a : \List A \to \Bool$ is defined as
$(\delta\,f\,a)(\vas) = f\, (a \tcons \vas)$.  This means that
$\delta\,f\,a$ accepts the words of $f$ that start with $a$, minus this
first letter.  In terms of tries $t$, we obtain the derivative
$\tdelta\,t\,a$ simply by following the $a$-labeled edge from the
root, thus, the derivation function is identical with the field
$\tdelta$.

There is one final twist to arrive at the Agda definition: In order to
facilitate corecursive definitions of tries that are certified by
Agda's productivity checker, we equip the type of tries $\Lang\,i$
with an index $i : \Size$.  Index $i$ denotes an ordinal $\leq \omega$
corresponding to the \emph{definedness depth} of a trie
$t : \Lang\,i$.  Ultimately, we are interested only in fully defined
tries $t : \Lang\,\tinfty$, where $\tinfty$ is syntax for ordinal
$\omega$.  This means we can query $t$'s nodes at arbitrary
depth.  For a finite definedness level $i$ we can only inspect
nodes up to depth $i$.  In particular $t : \Lang\,0$ allows us to look
only at the root label $\tnu\,t$, its subtrees via $\tdelta\,t\,a$ are
undefined and Agda's type checker will object to such an expression.

If for an arbitrary ordinal $i$,
a trie $t : \Lang\,i$ can be defined by reference to tries of type
$\Lang\,j$ for $j<i$, written $j : \SizeLt i$, then
$t$ can be assigned type $\forall\{i\} \to \Lang\,i$.
We say $t$ is defined by well-founded recursion on ordinal $i$,
which is our principle of corecursive definition.

\aLang

\noindent
As typing of the projections from tries we get
\[
\begin{array}{lll}
\tnu & : \forall\{i : \Size \} \to \Lang\,i \to \Bool \\
\tdelta & : \forall\{i : \Size\} \to \Lang\,i \to \forall\{j : \SizeLt i\} \to
          A \to \Lang\,j \\

\end{array}
\]
with hidden size arguments which will, if type checking succeeds, be
figured out by Agda's unifier and size constraint solver.
When taking the derivative $\tdelta\,\{i\}\,t\,\{j\}\,a$ we are free
to choose any $j$ strictly below $i$.  This expresses that if $t$ was
only defined up to depth $i$, then its $a$-subtree is less
defined; and we are allowed to waste information and choose a smaller
$j$ than necessary.  Wasting is fine since $\Lang\,i$ is a subtype of
$\Lang\,j$ whenever $i \geq j$, as we can always use a \emph{more} defined
value $t : \Lang\,i$ when a value of $\Lang\,j$ is demanded.  The
antitone subtyping chain
\[
  \Lang\,\tinfty \leq \dots \leq \Lang\,(\uparrow i) \leq \Lang\,i \leq
  \dots \Lang\,0
\]
can be justified by the equation
$\Lang\,i \cong \Bool \times \bigcap_{j<i} (A \to \Lang\,j)$.

The isomorphism $\Lang\,i \cong (\List\,i\,A \to \Bool)$ is witnessed by
the following two functions. The first, $l \tni \vas$, checks membership
of word $\vas$ in language $l$ represented as a trie.  The empty word
$\tnil$ is in the language if $\tnu\,l$, \ie, if the language is
nullable.  The composite
word $a \tcons \vas$ is accepted by $l$ if $\vas$ is accepted by the
derivative $\tdelta\,l\,a$.
\ani{}%
Visually spoken, $l \tni \vas$ returns the root label of the subtree
selected by path $\vas$.

The second function constructs a trie representation $\ttrie\,f$ from
the functional representation $f$ of a decidable language.  The trie
is constructed corecursively by copattern matching
\citep{abelPientkaThibodeauSetzer:popl13}.
\alang{}%
The root label $\tnu\,(\ttrie\,f)$
is determined by whether $f$ accepts the empty word $\tnil$.
The $a$-derivative $\tdelta\,(\ttrie\,f)\,a$ is constructed by
corecursion on the $a$-derivative of $f$.  The justification of the
recursive call to $\ttrie$ is apparent once we make the hidden size
arguments visible:
\[
  \tdelta\;\{i\}\;(\ttrie\,\{i\}\,f)\;\{j\}\;a = \ttrie\;\{j\}\;(\lambda \vas \to f\,(a \tcons \vas))
\]
By typing of the projection $\tdelta$, we have $j : \SizeLt i$, thus,
the definition of $\ttrie\,\{i\}$ only rests on $\ttrie\,\{j\}$ with a
smaller size index.  Well-founded induction on sizes guarantees that
the equation system has a unique solution.

The corecursive definition by copattern matching is sometimes likened
to differential equations \citep{hansenKupkeRutten:lmcs17}.  In the definition of
$\ttrie$, the second equation ($\tdelta$) is the differential equation, and the
first equation ($\tnu$) determines the initial value.

\subsection{Constructing decidable languages by coiteration}
\label{sec:coit}

In the following, we implement some standard constructions on formal
languages by copattern matching.  These operations will allow us to
compute the trie of any regular expression.
Throughout the rest of this article, we assume a type $\tA$ of
characters with a decidable equality
${\tlfloor a \qeq b \trfloor} : \Bool$ for $a, b : \tA$.
% $\_{\qeq}\_ : (a\; b : \tA) \to \Dec\,(a \tequiv b)$.  Herein,
% $\Dec\,P$ is a data type with two constructors: $\tyes$ carries a
% proof of $P$, and $\tno$ a refutation.

% \aDec

The empty language $\tempty$ is the trie where each node label is
$\tfalse$.  Naturally, each subtree of $\tempty$ is again $\tempty$.
\aempty%
The language $\teps$ accepting only the empty word has root label
$\ttrue$ but all other labels are $\tfalse$.  Hence, any derivative is
the empty language.
\aeps%
The language $\tchar\;a$ accepting the one-letter word $a \tcons \tnil$ is
not nullable, its $a$-derivative is $\teps$ and all other derivatives
are $\tempty$.
\achar%
We obtain the language complement $\tcompl\,l$ of a language $l$ by
flipping all labels.  This is accomplished by recursing (lazily) over the whole
tree.
\acompl%
Complement $\tcompl$ is a special instance of mapping a function
pointwise over all tree labels.

For the union $k \cup l$ of two languages $l$ and $k$, we overlay the
two tries and perform the Boolean disjunction operation on
corresponding node labels.
\aunion%
The intersection could be defined
analogously, using Boolean conjunction.  Both operations are instance
of a general $\tzipWith$-function that applies a binary operation
pointwise to a pair of tries.

All recursive definitions of tries so far have followed a specific pattern:
in the right hand sides of the recursive equations, the recursive call
was outermost, \ie, the equation had the form $\tdelta\,(g\,\vec y)\,x
= g\,\vec t$ for some variables $x,\vec y$ and some terms $\vec t$.
With the non-recursive equation being $\tnu\,(g\,\vec y) = o$,
this form is an instance of the commutative diagram for
terminal coalgebras and sometimes called \emph{coiteration}
\citep{geuvers:indtypes}.

For a functor $F : \Set \to \Set$, an $F$-coalgebra is a pair $(S,t)$
with $S : \Set$ and $t : S \to F\,S$.  An $F$-coalgebra morphism
between coalgebras $(S_1,t_1)$ and $(S_2,t_2)$ is a function
$f : S_1 \to S_2$ such that $t_2\,(f\, s_1)$ is equal to
$F\,f\,(t\,s_1)$ for all $s_1 : S_1$.  An $F$-coalgebra is terminal
if it is the target of a coalgebra morphism from every $F$-coalgebra.
Besides establishing the connection between coiteration and
coalgebras, we will not dwell on coalgebras in this article, thus, we
do not go into more details here.

Here is the diagram for a $(\Bool \times (A \to \_))$-coalgebra
$(\Gamma, h)$ mapping into the terminal coalgebra $(\Lang, \langle \tnu, \tdelta \rangle)$:
\[
% \newcommand{\tnd}{\langle o,\; \lambda x \to \vec t \rangle}
\newcommand{\tnd}{\,h}
\xymatrix@R=12ex@C=12ex{
  \Gamma \ar[r]^(0.45)*{\tnd} \ar@{.>}[d]_*{%g\,\vec y \,:=\,
    \tcoit\tnd}
    & *!<-1.6ex,0ex>{~\Bool \times (A \to \Gamma)} \ar[d]^*{\tid \times (\tcoit\tnd\, \circ \_)}\\
  \Lang \ar[r]^(0.45)*{\langle \tnu, \tdelta \rangle}
    & *!<-3.2ex,0ex>{~\Bool \times (A \to \Lang)} \\
}
\]
With $g := \tcoit\,h$, the commutative law
% $\langle \tnu, \tdelta \rangle \circ \tcoit\,h = \tid \times (\tcoit\,h\,
% \circ \_) \circ h$.
\[\langle \tnu, \tdelta \rangle \circ g = \tid \times (g \circ \_) \circ h\]
can be applied to points $\vec y : \Gamma$ to yield
\[\langle \tnu, \tdelta \rangle (g\,\vec y)
  = (\tid \times (g \circ \_)) (h\,\vec y).\]
For our instance, $h\, \vec y= (o,\, \lambda x \to \vec t)$
with $\vec y \of \Gamma \der o : \Bool$ and $\vec y \of \Gamma, x \of A \der
t : \Lang$, thus,
\[\langle \tnu, \tdelta \rangle (g\,\vec y)
  = (o,\; g \circ (\lambda x \to \vec t)).\]
This can be split into the two equations
\[
\begin{array}{lll}
  \tnu\,(g\,\vec y) & = & o \\
  \tdelta\,(g\,\vec y)\,x & = & g\,\vec t \\
\end{array}
\]
that form the laws of a function
$g = \tcoit\;(\lambda \vec y \to (o,\, \lambda x \to \vec t))$
defined by coiteration (modulo some tupling and (un)currying).

The type/context $\Gamma$ can be interpreted as the set of states of
an automaton $h$ with a coupled presentation of the accepting state
set $\Gamma \to \Bool$ and the transition function $\Gamma \to (A \to \Gamma)$.
Function $\tcoit\,h$ maps a state $s : \Gamma$ to the language
$\tcoit\,h\,s$ accepted by $h$ starting from state $s$.
The language constructions discussed at the beginning of this section
correspond to
constructions of (possibly infinite) automata
with references to existing automata as oracles. The
reader is invited to confirm this by expressing the given
constructions through coiteration.  Note however, that the state type $\Gamma$
might involve $\Lang$ and is, thus, not guaranteed to be finite!


\subsection{Constructing decidable languages by well-founded corecursion}

To complete the constructions of languages as supported by regular
expressions, we are missing language concatenation and the Kleene
star.  These can be constructed by \emph{corecursion up-to} which can
be reduced to primitive corecursion into a trie with an extended
alphabet \citep{traytel:fscd16}.  However, using sized types we can
naturally define these operations by their derivative laws, using
well-founded recursion on sizes.

Language concatenation $k \tdot l$ is our first non-trivial operation on languages.
The intuition
$(k \tni \vas) \wedge (l \tni \vbs) \imp (k \tdot l) \tni (\vas \oapp \vbs)$
leads to the specification
$(k \tdot l) \tni \vcs
  \iff \exists n \in \NN.\; k \tni (\ttake\,n\,\vcs) \wedge l \tni (\tdrop\,n\,\vcs)
$.\footnote{
We write $\vas \oapp \vbs$ for the concatenation of lists $\vas$ and
$\vbs$; we write $\ttake\,n\,\vcs$ for the largest prefix of $\vcs$ of
length $\leq n$, and $\tdrop\,n\,\vcs$ for the remainder. Note that
$\vcs = \ttake\,n\,\vcs \oapp \tdrop\,n\,\vcs$ for any $n \in \NN$.
}
However, this specification
does not directly suggest a pretty implementation of $k \tdot l$
\citep{doczkalKaiserSmolka:cpp13}.

We can instead try to understand language concatenation as an operation on the
tries $k$ and $l$.  If we think about accepting a word $\vcs$ in $k \tdot l$
by following paths in $k$ and $l$, the following procedure applies:
We start by following branches in $k$.  Whenever we reach an accepting
node in $k$ we may decide that we have reached the boundary between
the words $\vas$ in $k$ and $\vbs$ in $l$ that make up the word $\vcs
= (\vas \oapp \vbs)$ in $k \tdot l$. Hence, we start following branches in
$l$.  However, since we are not sure we already reached the boundary,
we simultaneously continue to follow branches in $k$.  At each
accepting node in $k$ we spawn off a run in $l$.  Thus, a trie for $k
\tdot l$ may be constructed by the following operation on all
accepting nodes of $k$:  make the node non-accepting but then union the
subtree starting here with $l$.  This transformation is achieved by
the following corecursive definition of concatenation:
\aconcat
The concatenation of two languages is nullable iff both are nullable.
For the $x$-derivative, we follow the $x$-branch in $k$
via $\tdelta\,k\,x \tdot l$ in any case.  If the node is accepting,
\ie, $\tnu\,k$ is $\ttrue$,
we may in addition follow the $x$-branch in $l$ via $\tdelta\,l\,x$.
These two cases are nicely combined with $\tapplyWhen$ from
\Cref{sec:agda} and section ${\_\tcup}\;\tdelta\,l\,x$
which is short for the function
$\lambda \, \kpl \to \kpl \tcuprel \tdelta\,l\,x$.
As before, the equations for language concatenation correspond to the
derivation laws of regular expressions \citep{brzozowski:jacm64},
but we arrived there by the trie intuition.

The above definition is not an instance of coiteration for two
reasons:  First, the outermost call is to $\tapplyWhen$
% $\tifthenelse$
rather than the recursive call %$\kpl$
$\tdelta\,k\,x \tdot l$.
%Even if we consider $\tifthenelse$ to be special
%(rather than just an arbitrary Agda function),
%there is still a
%recursive call $\kpl$ in the then-branch which is not at top-level,
Even when we inline $\tapplyWhen$,
the recursive call is not at the top level
but under the union-operator.  This
problem is usually fixed by defining a scheme for corecursion up to
union.  However, looking at the involved sizes we can accept the
definition in the present form as an instance of well-founded corecursion.
Crucial here is the sized typing of the union
\[
  \_\tcup\_ : \forall\{i\} (k\, l : \Lang\,i) \to \Lang\,i
\]
which asserts that the arguments are no deeper analyzed than the
definedness depth of the result.  If we make all hidden %size
arguments
visible---having to switch to prefix operators instead of infix ones---%
we can see the propagation of definedness depth levels to the
recursive call. % $\kpl$.
\aconcatexpl%
Since the recursive call happens at smaller index $j < i$, it is justified.
Note also that in the definition of $\kpl$, last letter, $l : \Lang\,i$ is
cast to $\Lang\,j$ which is a valid cast since $j < i$.

The iteration $l^*$ of a language $l$, aka \emph{Kleene star}, can be
informally described as ``zero or more words from $l$, concatenated''.  If for
some $n \geq 0$ we have words $\vas_1, \vas_2, \dots \vas_n \in l$, then
$(\vas_1 \oapp \vas_2 \oapp \dots \oapp \vas_n) \in l$.  In terms of tries, $l^*$
is obtained from $l$ by making the root accepting and unioning $l$
with any
subtree of $l$ that has an accepting root.  Intuitively, this means that at each
accepting node we may ``jump back'' to the root.  The corecursive definition
\astar%
relies on the sized typing of concatenation to justify the recursive call.

A variant of the Kleene star is the \emph{Kleene plus} $l^+ = l \cdot l^*$ meaning
``concatenation of one or more words from $l$''.
% This language is only nullable if $l$ is, otherwise the definition is like star.
% \aplus

This concludes our set of language operations defined by well-founded
corecursion.   These operations allow us to give an executable
semantics for regular expressions (leaving aside efficiency questions).
It may be remarked that, thanks to sized typing, all the definitions are concise
and direct counterparts of the derivative laws for regular expressions
\citep{brzozowski:jacm64}.

\section{Proving the Kleene Algebra Laws}
\label{sec:kleene}

In this section, we prove that decidable languages as introduced in
Section~\ref{sec:lang} form a \emph{Kleene algebra}.

\subsection{A family of equivalence relations over languages}
\label{sec:fameq}

Equality of tries, sometimes called \emph{strong bisimilarity}, is
defined coinductively as follows.  Two tries are strongly bisimilar if
they have the same root and corresponding subtries are strongly
bisimilar in turn.  In Agda, this amounts to the following coinductive
definition:
\abisim
Note that we are relating tries $l,k : \Lang\,\tinfty$ whose
definedness depth is unbounded ($\tinfty$).  This means that any
subtrie such as $\tdelta\,l\,a$ is defined and in turn has type
$\Lang\,\tinfty$.

However, the relation itself is indexed by a definedness depth $i$.
In fact we are defining a family of types such that $\bisim l j k$
is a subtype of $\bisim l i k$ whenever $i \leq j$.  The depth is a
lower bound on how far the proof of equality of $l$ and $k$ is
constructed.  In particular, we can only inspect the derivative
$\cdelta\,p\,a$ of a proof $p : \bisim l i k$ if $i > 0$.  As for
coinductive types like $\Lang\,i$, the size index $i$ is just a tool
for the corecursive construction of derivations.  Ultimately, we are
only interested in fully defined equality proofs $p : \bisim l \tinfty k$.
In particular, our size-index relation is not to be confused with \emph{ordered families
  of equivalences} (OFEs) \citep{gianantonioMiculan:types02} $l \equiv_n k$
which refine the notion of equality itself.  There, $l \equiv_0 k$
would hold always and $l \equiv_{n+1} k$ would hold if $l$ and $k$
have equal roots and their immediate subtries are $\equiv_n$-related.
The difference to sized types lies in the base case:
$\bisim l i k$ is \emph{undefined} for size $i = 0$, rather than being
trivially true.
OFEs are a different approach to justifying corecursive definitions.

Each of the coinductive relations forms an equivalence relation,
proven for the whole family by coiteration.  For reflexivity, we have
to prove that given a trie $l$, we can construct a derivation that $l$
is strongly bisimilar to itself $l$, up to arbitrary depth $i$.
\arefl
The proof $\tcongrefl$ of $\bisim l i l$ is constructed lazily.  If we
are asking for its first component $\congnu\,\tcongrefl$ we get a proof
that the root $\tnu\,l$ is identical to itself, namely $\trefl :
\tnu\,l \tequiv \tnu\,l$.  If we are asking for the $a$-branch of
its second component, $\congdelta\;\tcongrefl\;a$ at depth $j {<} i$, it
computes $\tcongrefl : \bisim {\tdelta\, l} j {\tdelta\, l}$ corecursively.

Symmetry is defined in a similar fashion.  To compute a proof of $k \cong l$
up to depth $i$, we only need a derivation of $l \cong k$ up to depth
$i$; thus, the type of $\tcongsym$ is $\bisim l i k \to \bisim k i l$.
\asym
Transitivity is likewise depth preserving.  Depth-preservation is
crucial to combine reasoning by transitivity and the coinductive
hypothesis in a natural way, as we will see below.
\atrans
Taken together, each $\bisim \_ i \_$ is an equivalence relation, and
forms a \emph{setoid}\footnotemark{} $\Bis\,i$ with carrier
$\Lang\,\tinfty$.
\footnotetext{A \emph{setoid} is a type with an equivalence relation
  on its elements.  In the Agda standard library, it is represented as
  a record with three fields:
\AgdaField{Carrier}, the type,
\AgdaField{$\_\approx\_$}, the relation, and
\AgdaField{isEquivalence}, the proof that
\AgdaField{$\_\approx\_$} is an equivalence relation.
%
We use setoids as a poor man's alternative to quotient types, which are
absent in Intensional Martin-Löf Type Theory and Agda.
}
\asetoid
Later, we will use these setoids to reason by equality chains.
Equality chains are not a built-in feature of Agda, but a module of
its standard library.  An equality chain allows us to write down
equational reasoning in a human-readable way, and is basically a nice
interface to reasoning by transitivity.  In general, it works for any
preorder, \ie, any reflexive-transitive relation.

Just for the sake of demonstration, we prove transitivity of
bisimilarity again, using the old transitivity proof in form of an
equality chain.

\atransp

As a prerequisite, we bring the primitives of equality chains into
scope by opening module $\EqR$ (short for $\EquationalReasoning$)
instantiated to the setoid $\Bis\,i$.
A chain then starts with $\tbegin$ followed with the first term of the
chain ($k$).  Then follows a justification ($p : \bisim k i l$) for
equality with the second term ($l$).  This may repeat for a while, in
our case, there is only another justification ($q : \bisim l i m$) and
a final term ($m$).  The chain closes with an end-of-proof maker ($\tqed$).


\subsection{Laws of language union}
\label{sec:lawunion}

Decidable languages form an idempotent commutative monoid under
union.  The individual laws, like associativity, commutativity,
idempotency, and unit, follow from the corresponding laws of the
Boolean disjunction, which are pointwise applied at all the
corresponding nodes of the involved tries.  In Agda, these are direct
proofs by coiteration.
\aunionassoc
\aunioncomm
\aunionidem
\aunionemptyl
% \aunionemptyr
Finally, union preserves equality, which is again proven by
coiteration.
The sized typing will be crucial to apply a coinductive hypothesis
under $\tunioncong$ later.
\aunioncong
A derived law we require later is that union distributes over itself.
Now that we have established that union fulfills the laws of an
idempotent commutative monoid, we can use a solver to prove this law
automatically by reflection.
\aunionuniondistr
Concretely, the solver checks that both sides of the equation have the
same set of atoms, by normalizing both sides to the set $\{k,l,m\}$.
This solver is implemented in Agda itself, but we will not describe it
further here.\footnote{%Implementation see
\url{https://github.com/agda/agda-stdlib/blob/1c78e4e/src/Algebra/IdempotentCommutativeMonoidSolver.agda}
implements this solver.
}

\subsection{Laws of language concatenation}
\label{sec:lawcat}

In this section, we prove laws of language concatenation $k \cdot l$.
Since it is defined by cases on whether $k$ is nullable, we will make
the same case distinction in most proofs.  To this end, we use Agda's
$\kwith$ construct, as for example in:
\awith
It can be roughly seen as a case distinction over $g\,x$, but it also
abstracts $g\,x$ in the goal $P\,(g\,x)$ so that we can solve it by $p
: P\,\ttrue$ in the first clause and $q : P\,\tfalse$ in the second clause.

Further, we use Agda's $\krewrite$ construct, which can be applied on
an equation $l \tequiv r$ to rewrite subterms $l$ in a goal to $r$.
For example:

\input{latex/RewriteExample}
\arewrite

Here, the goal is changed from $P\,(g\,x)$ to $P\,x$ using equation
$e$, and subsequently solved by $p$.

As a first law of concatenation, we consider
distributivity over union,
% Concatenation distributes over union,
for instance,
$k \tdot (l \tunion m) \cong (k \tdot l) \tunion (k \tdot m)$.
Naturally, we would like to prove this statement by coinduction.  The case for
$\tnu$ follows by the Boolean distributivity law
$x \wedge (y \vee z) = (x \wedge y) \vee (x \wedge z)$.  In the case
for $\tdelta$, we would like to reason by the following equality
chain.  We consider the subcase that $k$ is nullable, and underline
the subterms that have changed from the last line (unless the whole
expression has changed).
\[
\def\arraystretch{1.5}
\begin{array}{cl@{\qquad}r}
\derive{(k \tdot (l \tunion m))} a
  & \cong & \mbox{by definition} \\
\derive k a \tdot (l \tunion m) \tunion {\derive{(l \tunion m)} a}
  & \cong & \mbox{by definition} \\
{\derive k a \tdot (l \tunion m)} \tunion \underline{(\derive l a \tunion \derive m a)}
  & \cong & \mbox{by coinduction hypothesis} \\
\underline{(\derive k a \tdot l \tunion {\derive k a \tdot m})} \tunion ({\derive l a} \tunion \derive m a)
  & \cong & \mbox{by union laws} \\
{(\derive k a \tdot l \tunion \underline{\derive l a})} \tunion {(\underline{\derive k a \tdot m} \tunion \derive m a)}
  & \cong & \mbox{by definition} \\
\underline{\derive{(k \tdot l)} a} \tunion \underline{\derive{(k \tdot m)} a}
  & \cong & \mbox{by definition} \\
\derive{(k \tdot l \tunion k \tdot m)} a
\end{array}
\]
This proof does not follow the scheme of (primitive) coinduction. The
coinduction hypothesis is applied under uses of transitivity (for
connecting the equations) and under the congruence law for union.
This becomes especially clear if we fully write out the
justifications as in the corresponding Agda proof in
Figure~\ref{fig:concatuniondistribr}.  However, the continuity of
transitivity and $\tunioncongl$ as witnessed by the sized typing
justifies the use of the coinduction hypothesis.

% \begin{figure}[htbp]
%   \centering
% %\hrule width0.8\textwidth
% \aconcatuniondistribl
%   \caption{Concatenation distributes over union.}
%   \label{fig:concatuniondistribl}
% \end{figure}

\begin{figure}[htbp]
  \centering
%\hrule width0.8\textwidth
\aconcatuniondistribr
  \caption{Concatenation distributes over union.}
  \label{fig:concatuniondistribr}
\end{figure}

The other distributivity law is proven by coinduction and case
distinction over the nullability of $l$ and $k$.
\aconcatuniondistribl
Congruence laws for concatenation follow by coinduction and congruence
of union.
\aconcatcongl
\aconcatcongr
The coinductive proof of associativity relies on distributivity and
congruence and associativity of union.
\aconcatassoc
Finally, the empty language is a zero and the language of the empty
word a unit for language composition:
\aconcatemptyl
\aconcatemptyr
\aconcatunitl
\aconcatunitr

\subsection{Laws of the Kleene star}
\label{sec:lawstar}

The language of the empty word is the iteration of the empty language.

\astarempty

\noindent
To prove that iteration is idempotent, we first prove that
concatenation of iterated languages is idempotent.

\astarconcatidem

\noindent
This lets us prove idempotency of the Kleene star:

\astaridem

\noindent
The Kleene star obeys the following recursive equation:

\astarrec

\noindent
Finally, we prove Arden's rule \citeyearpar{arden:focs61},
which would allow us to solve linear
equations over regular expressions.
%\astarconcat
\astarfromrec

% \subsection{Laws of the Kleene plus}
% \label{sec:lawplus}

% The Kleene plus is definable from the star via $l^+ = l \cdot l^*$,
% this is the only fact we require:

% \aplusdef

Looking back, all our proofs about decidable languages
were performed rather mechanically using:
\begin{enumerate}\setlength{\itemsep}{0ex}
\item coinduction,
\item equality chains,
\item already proven lemmata.
\end{enumerate}
We did not require any up-to techniques or creative insight such as
finding bisimulation relations to carry out our proofs.
% that is closed under $\tcongnu$ and $\tcongdelta$.
Thus, it is likely that after initiating coinduction, standard
first-order theorem provers could fill in the remaining steps.


\section{Constructing Automata}
\label{sec:aut}

\input{latex/Automaton}

In this section, we show that deterministic automata form a Kleene
algebra like decidable languages do.  We show how to construct union,
concatenation, and Kleene star of automata, in a recapitulation of
the classic theory of formal languages.  Our message is that the
corresponding correctness proofs can be carried out by the same means
as in the last section: coinduction and equational reasoning.

In our presentation of deterministic automata (\DA) we follow
\cite{rutten:concur98}:  A not necessarily finite automaton over a
state set $S$ is given by a transition function
$\tdelta : S \to A \to S$ and a characteristic function
$\tnu : S \to \Bool$ for the
set of accepting (or final) states.  These two functions could also be
bundled as $S \to \Bool \times (A \to S)$, making apparent that an
automaton $(S : \Set,\ \vda : \DA\,S)$ is just a
$\Bool \times (A \to \_)$-coalgebra.

\aDA

In anticipation of power automata we lift the coalgebra to lists of
states $\List\,i\,S \to \Bool \times (A \to \List\,i\,S)$.  A list of
states is accepting ($\tnus$) if it contains at least one final state.  And we
step ($\tdeltas$) to a new list of states by pointwise applying the
transition function.
(Remember that $\tmap$ and $\tany$ have been defined in Section~\ref{sec:agdafun}.)

The initial state is not
contained in the automaton definition; each state $s$
induces a language $\tlang\;\vda\,s$
accepted by an automaton $\vda$, which can be defined by simple coiteration:

\aacclang

\noindent
For each automaton $(S, \vda)$ the function
$\tlang\;\vda : S \to \Lang\,\tinfty$ is the terminal morphism.
\[
\newcommand{\tnd}{\langle \DA.\tnu\,\vda,\ \DA.\tdelta\,\vda \rangle}
\xymatrix@R=12ex@C=20ex{
  S \ar[r]^(0.45)*{\tnd} \ar@{.>}[d]_*{%g\,\vec y \,:=\,
    \tlang\;\vda}
    & *!<-1.7ex,0ex>{~\Bool \times (A \to S)} \ar[d]^*{\tid \times (\tlang\;\vda\, \circ \_)}\\
  \Lang\,\tinfty \ar[r]^(0.45)*{\langle \Lang.\tnu, \Lang.\tdelta \rangle}
    & *!<-4.6ex,0ex>{~\Bool \times (A \to \Lang\,\tinfty)} \\
}
\]

\subsection{Simple constructions on automata}
\label{sec:simpleaut}

An automaton for the empty language can be constructed with a single
non-accepting state inhabiting Agda's unit type $\ttop$.

\begin{minipage}{0.5\linewidth}
\aemptyA
\end{minipage}
\begin{minipage}{0.5\linewidth}
\[
\xymatrix{
  \bullet \ar@(dr,ur)
}
\]
\end{minipage}

To recognize the language of the empty word, we use two states,
accepting $\ttrue$ and non-accepting $\tfalse : \Bool$.

\begin{minipage}{0.5\linewidth}
\aepsA
\end{minipage}
\begin{minipage}{0.5\linewidth}
\[
\xymatrix{
  *++[o][F=]{\ttrue } \ar[r] &
  *++[o][F-]{\tfalse} \ar@(ur,ul)
}
\]
\end{minipage}

To accept a the single letter word $a$, we have three states: an
initial state $\tinit$, an accepting state $\tacc$, and a rejecting
error state $\terr$.

\begin{minipage}{0.5\linewidth}
\acharA
\end{minipage}
\begin{minipage}{0.5\linewidth}
\[
\xymatrix{
  *++[o][F-]{\tinit } \ar[r]^{a} \ar[dr]_{\neg a} &
  *++[o][F=]{\tacc } \ar[d] \\
&
  *++[o][F-]{\terr} \ar@(dr,dl)
}
\]
\end{minipage}

Given an automaton $\vda$, we construct the automaton $\tcomplA\,\vda$
for the complement language by switching accepting and non-accepting states.

\acomplA

Given an automaton $\vda_1$ over state set $S_1$ accepting language $\ell_1$ and an
automaton $\vda_2$ over $S_2$ for $\ell_2$, we can recognize the union $\ell_1
\tunion \ell_2$ by the following \emph{product} automaton $\vda_1
\toplus \vda_2$ over state set $S_1 \times S_2$.  A state in the
product automaton is a pair of states $(s_1, s_2)$, one from each
original automaton.  Transitions are done in lock-step, and for
acceptance at least one of the original automata must be in a final
state.

\aunionA

\subsection{Automaton composition for language concatenation}
\label{sec:compaut}

In preparation for automaton constructions for language concatenation
and iteration, we define the power automaton, which allows us to be in
a set of states at the same time.  It is actually sufficient to
consider finite sets of states, which we represent a bit redundantly
as lists.

\apowA

If we start the power automaton in state $[s_1,\dots,s_n]$, the
accepted language will be the $\bigcup_{i=1}^n \tlang\,\vda\,s_i$.  We
prove this in two steps:
First, if we start out in no states, the accepted language is empty.

\apowAnil

If we start in the non-empty list $s \tcons \varss$, we accept the union of
the accepted language of $\vda$ from $s$ and the accepted language of
$\tpowA\,\vda$ from $\varss$.

\apowAcons

For language concatenation, given two automata $\vda_1$ and $\vda_2$,
we will construct a composition automaton
$\tcomposeA\,\vda_1\,s_2\,\vda_2$ such that its accepted language from
state $s_1$ is the language concatenation
$\tlang\,\vda_1\,s_1 \tdot \tlang\,\vda_2\,s_2$.
The key insight is that whenever we reach a final state $s_f$ in $\vda_1$, we
non-deterministically jump to the initial state $s_2$ of $\vda_2$.
In some formulations of non-deterministic automata this would be
an $\varepsilon$-transition
% a silent transition
from $s_f$ to $s_2$, consuming no input.
We will instead add transitions from $s_f$ to the successor states of $s_2$.
\[
\xymatrix{
& & \ar@{--}[dddd] & & \\
\bullet & &  & & \bullet \\
\vda_1 % \ar@{.>}[r]
& *++[o][F=]{s_f}
   \ar[ul]_a
   \ar[dl]^b
   \ar@{.>}[rr]^(0.4){\varepsilon}
   \ar@{.>}@/^/[rrru]^a
   \ar@{.>}@/_/[rrrd]_b
&
& *++[o][F-]{s_2} \ar[ur]^a \ar[dr]_b
& \vda_2 % \ar@{.>}[l]
\\
\bullet & & & & \bullet \\
& & & & \\
}
\]

This means for the composition that
we are in one state of $\vda_1$ and in zero or more states
of $\vda_2$ at the same time.  Thus, the type of states is
$S_1 \times \List\,\tinfty\,S_2$ and we consider the power of the
second automaton.

\acomposeA

A state $(s_1, \varss_2)$ of the composition automation is final if any
of $\varss_2$ is final, or if $s_1$ is final and the initial state $s_2$
of $\vda_2$ is also a final state.  (The latter means that the second
language is nullable, so any word of the first language is contained
in the composition.)

\acomposeAnu

To step from state $(s_1, \varss_2)$ we consider two cases.  First, if
$s_1$ is not final, we simply transition pointwise, from $s_1$ with
$\tdelta\, \vda_1$, and from each state in $\varss_2$ with $\tdelta\,\vda_2$.
However, if $s_1$ is final, we imagine to be also in the initial
state $s_2$ of $\vda_2$, thus, we add to this the transition we can
make from $s_2$ in the second automaton.

\acomposeAdelta

The composition automaton is a non-trivial construction, thus, it
makes sense to look at its correctness proof.  We have to generalize
the correctness statement to arbitrary initial states $(s_1, \varss)$ in
the composition automaton.  If $\varss$ is not empty, the accepted
language of the composition automaton
contains the union of the accepted languages from each state
in $\varss$ as well.

\acomposeAgen

The proof is by coinduction, using lemma $\tpowAcons$ in case $s_1$ is
final.

\acomposeAgenproof

As a corollary for empty $\varss$, we obtain the correctness of automaton composition:

\acomposeAcorrect


\subsection{Automaton construction for language iteration}
\label{sec:staraut}

Constructing the Kleene star of an automaton $\vda$ with initial state
$s_0 : S$ can be factored into two steps:
\[
\xymatrix{
&&&& \bullet
\\
\ar@{.>}[r]
&*++[o][F=]{\tnothing}
  \ar@{.>}@/^/[rrru]^a
  \ar@{.>}@/_/[rrd]_b
&
& *++[o][F-]{s_0}
  \ar[ur]^a
  \ar[d]_b
& \vda~~~~~
& *++[o][F=]{}
  \ar@{.>}@/_/[ul]_a
  \ar@{.>}[dll]^b
\\
&&& \bullet
&& *++[o][F=]{}
  \ar@{.>}@(ul,d)[uul]_>>>>>>{a}
  \ar@{.>}[ll]^b
\\
}
\]
\begin{enumerate}
\item $\tplusA$: Construct the \emph{Kleene plus} %$\tplusA\,vda$
  by adding the
  successors of $s_0$ to those of each final state.
  This enables repetition after the first run.
  At this point, the automaton becomes
  ``non-deterministic'', \ie, we switch to $\List\,\tinfty\,S$.
  \aplusA%

\item $\tacceptingInitial$:
  Add a new final state
  $\tnothing : \Maybe\,S'$
  (where $S' = \List\,\tinfty\,S$)
   with the same successors as
  $s_0$.  State $\tnothing$ will serve as the new initial state.  Its
  finality guarantees that the empty word is accepted.
  \aaccinitnothing%
  Old states $s : S'$ are embedded as $\tjust\,s : \Maybe\,S'$.
  \aaccinitjust
\end{enumerate}

\astarA

The main lemma characterizes the language accepted by
$\tplusA\,s_0\,\vda$ from an arbitrary state $\varss$.

\aplusAlemma

The proof by coinduction uses $\tpowAcons$
and some laws of decidable languages as proven in
Section~\ref{sec:lang}.

When we instantiate $\varss$ to the singleton $s_0$, we get the
correctness of the $\tplusA$ construction:

\aplusAcorrect

% Finally, from an automaton $\vda$ accepting language
% $\ell$ from state $s_0 : S$,  we construct an
% automaton $\tstarA\,\vda$ for the iterated language $\ell^*$.
% We do this in two steps:
% \begin{enumerate}
% \item $\tacceptingInitial$: Add a new final state $\tnothing : \Maybe\,S$ with the same successors as
%   $s_0$.  State $\tnothing$ will serve as the new initial state.  Its
%   finality guarantees that the empty word is accepted.
% \item $\tfinalToInitial$: Add the successors of $s_0$ to each final state.  This enables
%   iteration.  At this point, the automaton becomes
%   ``non-deterministic'', \ie, we switch to $\List\,\tinfty\,(\Maybe\,S)$.
% \end{enumerate}

% \noindent
% The first step embeds states $s : S$ of $\vda$ as $\tjust\,s : \Maybe\,S$.

% \aaccinit

% \noindent
% It adds the new accepting state $\tnothing : \Maybe\,S$ with the
% successors of $s_0$.

% \aaccinitnothing

% \noindent
% The second step constructs the power automaton and adds transitions
% from the final states to the successors of the initial state.

% \afinalinit

% \noindent
% Composing these steps leads to the automaton for language iteration.

% \astarA

To verify the $\tstarA$ construction, we observe
for  $\tacceptingInitial$
that embedding the states via
$\tjust : S' \to \Maybe\,S'$ does not change the accepted language.

\aacceptingInitialjust

\noindent
This lemma is proven directly by coinduction.

Further, the language accepted by the new state $\tnothing : \Maybe\,S$
is the language accepted by $s_0$ enriched with the empty word.

\aacceptingInitialnothing

\noindent
The proof by coinduction uses $\tacceptingInitialjust$.

Finally, we prove correctness of the $\tstarA$-construction:  If we
start in the new initial state $\tnothing$, the recognized
language is the Kleene star of the language recognized by $\vda$ from $s_0$.

\astarAcorrect

The proof is direct by correctness $\tplusAcorrect$ of the Kleene plus
construction, lemma $\tacceptingInitialnothing$,
and the fact \tstarrec that $l^* = \Ge \cup l^+$.


% TODO: Discuss related work

% > [1] Thomas Braibant, Damien Pous: An Efficient Coq Tactic for Deciding
% > Kleene Algebras. ITP 2010: 163-178


% > [2] Thierry Coquand, Vincent Siles: A Decision Procedure for Regular
% > Expression Equivalence in Type Theory. CPP 2011: 119-134


% > [3] Alexander Krauss, Tobias Nipkow: Proof Pearl: Regular Expression
% > Equivalence and Relation Algebra. J. Autom. Reasoning 49(1): 95-106
% > (2012)


% > [4] Damien Pous: Kleene Algebra with Tests and Coq Tools for while
% > Programs. ITP 2013: 180-196 (idempotent commutative monoid solver)


% > [5] Lawrence C. Paulson: A Formalisation of Finite Automata Using
% > Hereditarily Finite Sets. CADE 2015: 231-245


% > [6] Jurriaan Rot, Marcello M. Bonsangue, Jan Rutten: Proving language
% > inclusion and equivalence by coinduction. Inf. Comput. 246: 62-76
% > (2016)


% > [7] Dmitriy Traytel: Formal Languages, Formally and
% > Coinductively. FSCD 2016: 31:1-31:17


% > [8] Alasdair Armstrong, Georg Struth, Tjark Weber: Programming and
% > automating mathematics in the Tarski-Kleene
% > hierarchy. J. Log. Algebr. Meth. Program. 83(2): 87-102 (2014)


% > [9] Chung-Kil Hur, Georg Neis, Derek Dreyer, Viktor Vafeiadis: The
% > power of parameterization in coinductive proof. POPL 2013: 193-206


% > [10] Damien Pous: Coinduction All the Way Up. LICS 2016: 307-316


% > [11] Jasmin Christian Blanchette, Aymeric Bouzy, Andreas Lochbihler,
% > Andrei Popescu, Dmitriy Traytel: Friends with Benefits - Implementing
% > Corecursion in Foundational Proof Assistants. ESOP 2017: 111-140



\section{Conclusions and Too Much Related Work}
\label{sec:concl}

In this article, we have demonstrated that well-founded coinduction
realized by sized types and copattern matching allows for elegant
definitions of decidable languages, language operations, and
correctness proofs for automata constructions.  All definitions and
proofs could be carried out formally in the Agda proof assistant,
using standard tools like equation chains and a simple monoid solver.

Beyond the material presented in this article, we have also formalized
regular expressions and their equivalence to regular (Chomsky type~3)
grammars, using the same proof techniques.

Being one of the oldest topics of computer science and taught to every
student, there is an abundance of related work we are not able to
review here.  We just wish to mention a recent and comprehensive
Coq formalization of
classic automata theory by
\cite{doczkalKaiserSmolka:cpp13}.  In contrast to us, they properly
restrict to \emph{finite} automata, using the support for finite types
given by the SSReflect library \citep{gonthierMahboubi:ssreflect}.

\para{Acknowledgments}
The author acknowledges support from Vetenskapsr\aa{}det
(Swedish Research Council) through project 621-2014-4864/E0486401
\emph{Termination
Certification for Dependently-Typed Programs and Proofs via Refinement
Types} and from the
COST Action CA15123 \emph{European research network on types for programming
and verification} (EUTYPES).
He is grateful to Ichiro Hasuo and the program committee of the IFIP
WG 1.3 International Workshop on Coalgebraic Methods in Computer
Science, CMCS 2016, for an invitation to present his findings at this
workshop.

\section*{References}

\bibliographystyle{elsarticle-harv}
\bibliography{auto-jlamp17}

\end{document}

\endinput
%%
%% End of file
